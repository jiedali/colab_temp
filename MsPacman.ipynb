{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MsPacman.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCiZwohgjyBrcDGFbxj+rW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiedali/colab_temp/blob/main/MsPacman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UBY6LaZYFEJ",
        "outputId": "f3c8eaf7-bae8-4ff8-c84a-2c4efd4a54a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# imports\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import gym\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "# choose a GPU card\n",
        "# os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
        "# Set seed for tensorflow\n",
        "SEED=123\n",
        "tf.set_random_seed(SEED)\n",
        "GYM_SEED=678"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOi9tay-ZQ5l",
        "outputId": "e1fd3cde-1ddf-431d-e094-da655ce4485a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make(\"MsPacman-v0\")\n",
        "obs = env.reset()\n",
        "obs.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 160, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfvPCQi02c0u",
        "outputId": "2bec78e7-7dde-4681-a6cf-3736c0d71f13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppZw7FFieGrh"
      },
      "source": [
        "mspacman_color = 210 + 164 + 74\n",
        "def preprocess(obs):\n",
        "    img = obs[1:176:2, ::2] # crop and downsize\n",
        "    img = img.sum(axis=2) # to greyscale\n",
        "    img[img==mspacman_color] = 0 # Improve contrast\n",
        "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
        "    return img.reshape(88, 80, 1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbSzgnp3eH9r"
      },
      "source": [
        "result=preprocess_observation(obs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpymfaWZeK4l",
        "outputId": "1fbaf16a-2a26-41df-8de1-859a5ce7c782",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88, 80, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pbSRYn5eLxT"
      },
      "source": [
        "# This is now exactly the same as Ghani's CNN settings\n",
        "# need to adjust to 80**)\n",
        "learning_rate=0.00025\n",
        "# which is height? Which is width?\n",
        "state_size=[88,80,1]\n",
        "action_size=env.action_space.n\n",
        "n_outputs=action_size \n",
        "input_height=88\n",
        "input_width=80\n",
        "input_channels=1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQGTweyU2YIx",
        "outputId": "c635f204-38b2-4c34-eade-1720ca1f1a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define a Q network with input size of [88,80], and an output size of size of action space 2\n",
        "# Note that the action space is 4 with following meanings:\n",
        "# ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
        "# We will only be taking action 2 or 3\n",
        "class DQN(object):\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='online_q_network'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.name =name\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            with tf.name_scope(\"inputs\"):\n",
        "                # Jieda note: state is preprocessd 80*80*1 array\n",
        "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
        "\n",
        "            with tf.name_scope(\"conv1\"):\n",
        "                self.conv1 = tf.layers.conv2d(\n",
        "                inputs=self.inputs, filters=32, kernel_size=[8, 8], strides=4,\n",
        "                kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "                padding=\"VALID\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
        "                \n",
        "                self.conv1_out = tf.nn.relu(self.conv1, name='conv1_out')\n",
        "             \n",
        "            with tf.name_scope(\"conv2\"):\n",
        "                \n",
        "                self.conv2 = tf.layers.conv2d(\n",
        "                inputs = self.conv1_out, filters=64,\n",
        "                kernel_size=[4,4], strides=[2,2],padding=\"VALID\",\n",
        "                kernel_initializer = tf.variance_scaling_initializer(scale=2),\n",
        "                activation=tf.nn.relu, use_bias=False, name='conv2')\n",
        "                \n",
        "                self.conv2_out = tf.nn.relu(self.conv2, name='conv2_out')\n",
        "            \n",
        "            with tf.name_scope(\"conv3\"):\n",
        "            \n",
        "                self.conv3 = tf.layers.conv2d(\n",
        "                inputs = self.conv2_out, filters=64,\n",
        "                 kernel_size=[3,3], strides=[1,1], padding=\"VALID\",\n",
        "                 kernel_initializer = tf.variance_scaling_initializer(scale=2),\n",
        "                 name = \"conv3\")\n",
        "                \n",
        "                self.conv3_out = tf.nn.relu(self.conv3, name='conv3_out')\n",
        "            \n",
        "            with tf.name_scope(\"flatten\"):\n",
        "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "                \n",
        "\n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
        "                                          units = 512, activation = tf.nn.relu,\n",
        "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
        "            \n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc2 = tf.layers.dense(inputs=self.flatten,\n",
        "                                          units = 512, activation = tf.nn.relu,\n",
        "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc2\")\n",
        "\n",
        "            with tf.name_scope(\"outputs\"):\n",
        "                self.outputs = tf.layers.dense(inputs = self.fc2,\n",
        "                                              units = action_size,\n",
        "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                              activation = None)\n",
        "            # Output is the approximated Action Values Q(s,a), so we don't need any activation \n",
        "\n",
        "    def get_outputs(self):\n",
        "      \n",
        "        return self.outputs\n",
        "\n",
        "    def get_weights(self,scope_name):\n",
        "        # give all the weights of that network\n",
        "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name)\n",
        "        # create a dictionary to contain the values of the network weights\n",
        "        trainable_vars_by_name = {var.name[len(scope_name):]: var\n",
        "                                for var in trainable_vars}\n",
        "        return trainable_vars_by_name  \n",
        "\n",
        "#\n",
        "tf.reset_default_graph()\n",
        "X_state=tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
        "# initialize the two Q network\n",
        "online_q = DQN(state_size, action_size, learning_rate, 'online_q_network')\n",
        "target_q = DQN(state_size, action_size, learning_rate, 'target_q_network')\n",
        "# get the output and weights from online q network\n",
        "online_q_values=online_q.get_outputs()\n",
        "online_q_weights=online_q.get_weights(scope_name='online_q_network')\n",
        "# # get the output and weights from target q network\n",
        "target_q_values=target_q.get_outputs()\n",
        "target_q_weights=target_q.get_weights(scope_name='target_q_network')\n",
        "# define the operation to copy the online network weights to target_q_network weights\n",
        "copy_ops = [target_var.assign(online_q_weights[var_name]) for var_name, target_var in target_q_weights.items()]\n",
        "#\n",
        "copy_online_to_target = tf.group(*copy_ops)\n",
        "\n",
        "## define the train operation\n",
        "with tf.variable_scope(\"train\"):\n",
        "  # define training operation\n",
        "  input_action = tf.placeholder(tf.int32, shape=[None])\n",
        "  y=tf.placeholder(tf.float32, shape=[None, 1])\n",
        "  # Get the Q values for the input_action\n",
        "  q_value = tf.reduce_sum(online_q_values*tf.one_hot(input_action,action_size),axis=1,keepdims=True)\n",
        "  # compute the error between lable y and q_value from online_q_network approximation\n",
        "  # Note that lable y is computed using the target_q_network\n",
        "  error=tf.abs(y-q_value)\n",
        "  clipped_error = tf.clip_by_value(error,0,1)\n",
        "  linear_error = 2*(error-clipped_error)\n",
        "  loss = tf.reduce_mean(tf.square(clipped_error)+linear_error)\n",
        "\n",
        "  # global_step is used to keep track of number of training steps completed\n",
        "  global_step = tf.Variable(0,trainable=False,name='global_step')\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
        "  training_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "init=tf.global_variables_initializer()\n",
        "saver=tf.train.Saver()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-bb038b9163a6>:21: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-19-bb038b9163a6>:52: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bEie_tZ2tTf"
      },
      "source": [
        "replay_memory_size=50000\n",
        "# replay_memory = deque([],maxlen=replay_memory_size)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self,maxlen):\n",
        "        self.maxlen=maxlen\n",
        "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
        "        self.index=0\n",
        "        self.length=0\n",
        "\n",
        "    def append(self, data):\n",
        "        self.buf[self.index] = data\n",
        "        self.length = min(self.length+1, self.maxlen)\n",
        "        self.index = (self.index + 1) % self.maxlen\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # sample without replacement\n",
        "        indices = np.random.permutation(self.length)[:batch_size]\n",
        "        return self.buf[indices]\n",
        "\n",
        "# create an instance of ReplayBuffer\n",
        "replay_buffer = ReplayBuffer(replay_memory_size)\n",
        "def sample_replay_buffer(batch_size):\n",
        "  cols =[[],[],[],[],[]] # state, action, reward, next_state, continue\n",
        "  for memory in replay_buffer.sample(batch_size):\n",
        "    for col, value in zip(cols, memory):\n",
        "      col.append(value)\n",
        "  cols = [np.array(col) for col in cols]\n",
        "  return cols[0], cols[1], cols[2].reshape(-1,1), cols[3], cols[4].reshape(-1,1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26iXdvC-23k9"
      },
      "source": [
        "eps_min = 0.1\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 2000000\n",
        "def epsilon_greedy(q_values, step):\n",
        "    # Note: we gradually decrease epsilon, we explore more in the beginning, less towards later\n",
        "    epsilon = max(eps_min, eps_max-(eps_max-eps_min)*step/eps_decay_steps)\n",
        "    if np.random.rand()<epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return np.argmax(q_values)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJgiVGjx25k5"
      },
      "source": [
        "# This is the number of training steps \n",
        "n_steps = 4000000\n",
        "# In the very beginning, First have 10000 samples in the replay buffer\n",
        "training_start=10000\n",
        "# Before each training step, we run 4 episodes and add those samples to replay buffer\n",
        "training_interval=4\n",
        "# \n",
        "save_steps=1000\n",
        "copy_steps = 2500 # copy the online network parameters to target network every 10000 steps\n",
        "discount_rate=0.99\n",
        "skip_start=90\n",
        "batch_size=50\n",
        "iteration=0\n",
        "done=True\n",
        "# initialize the tracking statistics\n",
        "max_episode_reward_so_far=0\n",
        "total_max_q=-float('inf')\n",
        "mean_reward=0\n",
        "total_reward=0\n",
        "game_length=0\n",
        "game_counter=0"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvNZ_LHSS8kH"
      },
      "source": [
        "checkpoint_path=\"./mspacman_dqn.ckpt\""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHJgyhgm2-tt",
        "outputId": "e07d9a29-11ab-46b8-cac9-cce022bfd9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # if os.path.isfile(checkpoint_path +\".index\"):\n",
        "  #   saver.restore(sess, checkpoint_path)\n",
        "  # else:\n",
        "  init.run()\n",
        "  # to make sure they have the same values to begin with\n",
        "  # otherwise, different seeds will result in different initialized network weights\n",
        "  copy_online_to_target.run()\n",
        "  # intialize replay buffer\n",
        "  replay_buffer = ReplayBuffer(replay_memory_size)\n",
        "  # start training\n",
        "  # Training loop : \n",
        "  # (1) collect M data points, add to replay buffer (play 4 episodes of game, add those to replay buffer)\n",
        "  # (2) copy online network parameter to target network\n",
        "  # (3) sample a batch from replay buffer, do the online network update (batch size:50)\n",
        "  while True:\n",
        "    step = global_step.eval()\n",
        "    # if global training steps have meet the maximum, we will stop training\n",
        "    if step >= n_steps:\n",
        "      break\n",
        "    # One iteration is just one sample generation !!!\n",
        "    iteration+=1\n",
        "    if done:\n",
        "      obs = env.reset()\n",
        "      for skip in range(skip_start): # skip the beginning of each game\n",
        "        obs, reward, done, info = env.step(0) # take action 0\n",
        "      state = preprocess(obs)\n",
        "      \n",
        "    state = state.reshape(1,88,80,1)\n",
        "    state_to_rb=state.reshape(88,80,1)\n",
        "      \n",
        "    # online network evaluates what to do\n",
        "    q_values = online_q_values.eval(feed_dict={online_q.inputs:state})\n",
        "    # q_values returns array of 4 values corresponding to 4 actions\n",
        "    # for each step in the game, we record the max of q_values \n",
        "    action = epsilon_greedy(q_values,step)\n",
        "\n",
        "    # online network plays\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = preprocess(obs)\n",
        "    next_state = next_state.reshape(1,88,80,1)\n",
        "    next_state_to_rb = next_state.reshape(88,80,1)\n",
        "\n",
        "    # Add this sample to the replay buffer\n",
        "    replay_buffer.append((state_to_rb, action, reward, next_state_to_rb, 1.0-done))\n",
        "    # pass \"next_state\" to \"state\"\n",
        "    state=next_state\n",
        "    state_to_rb=next_state_to_rb\n",
        "\n",
        "    # compute statistics to track training progress\n",
        "    # track the maximum episode reward obtained so far, and the average episode reward\n",
        "    total_reward += reward\n",
        "    total_max_q += q_values.max()\n",
        "    game_length +=1\n",
        "    if done:\n",
        "      game_counter +=1\n",
        "      mean_max_q = total_max_q/game_length\n",
        "      mean_reward = total_reward/game_length\n",
        "      if total_reward > max_episode_reward_so_far:\n",
        "        max_episode_reward_so_far = total_reward\n",
        "      total_max_q=0.0\n",
        "      game_length = 0\n",
        "      total_reward=0\n",
        "\n",
        "    # if number of samples are less than training_start(10000), or it is not a multiple of 4, we will NOT do training\n",
        "    if iteration < training_start or iteration % training_interval !=0:\n",
        "      continue\n",
        "    \n",
        "    if iteration % 1000 == 0: # print stats every 100 training steps\n",
        "      print(\"Iteration %d\\tTraining step %d/%d  Mean Max Q %f episode total reward %d  Max episode reward so far %d  Game length %d  Game counts %d\" %(iteration, step, n_steps, mean_max_q, total_reward, max_episode_reward_so_far, game_length, game_counter))\n",
        "    # otherwise, if iteration is more than 10,000 and it is a multiple of 4, we will update the network\n",
        "    # result=sample_replay_buffer(batch_size)\n",
        "    X_state_val, X_action_val, rewards, X_next_state_val, continues = sample_replay_buffer(batch_size)\n",
        "    #\n",
        "    next_q_values = target_q_values.eval(\n",
        "        feed_dict = {target_q.inputs: X_next_state_val}\n",
        "    )\n",
        "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
        "    y_val = rewards + continues*discount_rate*max_next_q_values\n",
        "\n",
        "    # train online q network\n",
        "    _, loss_val = sess.run([training_op,loss], feed_dict={online_q.inputs:X_state_val, input_action: X_action_val, y:y_val})\n",
        "\n",
        "    # copy online q network to target network\n",
        "    if step % copy_steps ==0:\n",
        "      copy_online_to_target.run()\n",
        "    \n",
        "    # save dqn regularly\n",
        "    if step % save_steps ==0:\n",
        "      saver.save(sess,checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 11000\tTraining step 249/4000000  Mean Max Q 219.862031 episode total reward 120  Max episode reward so far 310  Game length 329  Game counts 19\n",
            "Iteration 12000\tTraining step 499/4000000  Mean Max Q 216.795004 episode total reward 130  Max episode reward so far 310  Game length 281  Game counts 21\n",
            "Iteration 13000\tTraining step 749/4000000  Mean Max Q 217.582171 episode total reward 110  Max episode reward so far 310  Game length 122  Game counts 23\n",
            "Iteration 14000\tTraining step 999/4000000  Mean Max Q 214.505654 episode total reward 160  Max episode reward so far 310  Game length 487  Game counts 24\n",
            "Iteration 15000\tTraining step 1249/4000000  Mean Max Q 223.359445 episode total reward 30  Max episode reward so far 310  Game length 42  Game counts 27\n",
            "Iteration 16000\tTraining step 1499/4000000  Mean Max Q 221.248645 episode total reward 230  Max episode reward so far 310  Game length 400  Game counts 28\n",
            "Iteration 17000\tTraining step 1749/4000000  Mean Max Q 224.211569 episode total reward 80  Max episode reward so far 370  Game length 311  Game counts 30\n",
            "Iteration 18000\tTraining step 1999/4000000  Mean Max Q 207.493345 episode total reward 200  Max episode reward so far 370  Game length 286  Game counts 32\n",
            "Iteration 19000\tTraining step 2249/4000000  Mean Max Q 219.396775 episode total reward 70  Max episode reward so far 370  Game length 192  Game counts 34\n",
            "Iteration 20000\tTraining step 2499/4000000  Mean Max Q 221.898139 episode total reward 270  Max episode reward so far 370  Game length 624  Game counts 35\n",
            "Iteration 21000\tTraining step 2749/4000000  Mean Max Q 213.376026 episode total reward 200  Max episode reward so far 370  Game length 347  Game counts 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te8Rml2ZS_Vn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}