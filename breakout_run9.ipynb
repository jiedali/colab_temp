{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "breakout_run9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN7OFOv1FFElYNhV89VLuqg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiedali/colab_temp/blob/main/breakout_run9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTPQOEP5Fw8B",
        "outputId": "fa488111-bb87-4be1-90b4-cb68de1073c5"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import gym\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "# choose a GPU card\n",
        "# os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
        "# Set seed for tensorflow\n",
        "# SEED=123\n",
        "# tf.set_random_seed(SEED)\n",
        "# GYM_SEED=678"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHqdUfl9F02p",
        "outputId": "d475dd48-1baf-4341-cb2f-1352556354c0"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov 20 16:30:32 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YwtJZv6F5JY",
        "outputId": "819416bd-7093-49d4-b824-09155a8d417e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCe5NvWEHBQ7"
      },
      "source": [
        "def preprocess_observation(image):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
        "    image = image[35:195] # crop\n",
        "    image = image[::2,::2,0] # downsample by factor of 2\n",
        "    image[image == 144] = 0 # erase background (background type 1)\n",
        "    image[image == 109] = 0 # erase background (background type 2)\n",
        "    image[image != 0] = 1 # everything else just set to 1\n",
        "    return np.reshape(image.astype(np.float).ravel(), [80,80,1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFGtBs6EVDj8"
      },
      "source": [
        "eps_min = 0.1\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 2000000\n",
        "n_outputs = 4\n",
        "\n",
        "replay_memory_size = 250000\n",
        "replay_memory = deque([], maxlen=replay_memory_size)\n",
        "\n",
        "\n",
        "def sample_memories(batch_size):\n",
        "    indices = np.random.permutation(len(replay_memory))[:batch_size]\n",
        "    cols = [[], [], [], [], []]\n",
        "    for idx in indices:\n",
        "        memory = replay_memory[idx]\n",
        "        for col, value in zip(cols, memory):\n",
        "            col.append(value)\n",
        "    cols = [np.array(col) for col in cols]\n",
        "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)\n",
        "\n",
        "\n",
        "def epsilon_greedy(q_values, step):\n",
        "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return np.argmax(q_values)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzuOwYL0VGTS"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self):\n",
        "        self.input_height = 80\n",
        "        self.input_width = 80\n",
        "        self.input_channels = 1\n",
        "        self.conv_n_maps = [32, 64, 64]\n",
        "        self.conv_kernel_sizes = [(8, 8), (4, 4), (3, 3)]\n",
        "        self.conv_strides = [4, 2, 1]\n",
        "        self.conv_paddings = [\"SAME\"] * 3\n",
        "        self.conv_activation = [tf.nn.relu] * 3\n",
        "        # The output from conv3 layer, is 64 filters, and the resulting shape for each filter is 10*10 (given by the SAME padding)\n",
        "        self.n_hidden_in = 64 * 10 * 10\n",
        "        self.n_hidden = 512\n",
        "        self.hidden_activation = tf.nn.relu\n",
        "        self.n_outputs = 4\n",
        "        self.initializer = tf.contrib.layers.variance_scaling_initializer()\n",
        "\n",
        "    def _zipped_params(self):\n",
        "        return zip(self.conv_n_maps, self.conv_kernel_sizes,\n",
        "                   self.conv_strides, self.conv_paddings, self.conv_activation)\n",
        "\n",
        "    def create_model(self, state, name):\n",
        "        prev_layer = state / 128.0\n",
        "        with tf.variable_scope(name) as scope:\n",
        "            for n_maps, kernel_size, strides, padding, activation in self._zipped_params():\n",
        "                prev_layer = tf.layers.conv2d(prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
        "                                              strides=strides, padding=padding, activation=activation,\n",
        "                                              kernel_initializer=self.initializer)\n",
        "\n",
        "            last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, self.n_hidden_in])\n",
        "            hidden = tf.layers.dense(last_conv_layer_flat, self.n_hidden, activation=self.hidden_activation,\n",
        "                                     kernel_initializer=self.initializer)\n",
        "            outputs = tf.layers.dense(hidden, self.n_outputs, kernel_initializer=self.initializer)\n",
        "\n",
        "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
        "        trainable_vars_by_name = {var.name[len(scope.name):]: var for var in trainable_vars}\n",
        "\n",
        "        return outputs, trainable_vars_by_name"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "q5kMtNa4VK2q",
        "outputId": "2f72c936-7728-49bc-d425-0a1b961d8cde"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "input_height = 80\n",
        "input_width = 80\n",
        "input_channels = 1\n",
        "n_outputs = 4\n",
        "\n",
        "learning_rate = 0.00025\n",
        "# momentum = 0.95\n",
        "# parameters for. RMSProp\n",
        "rmsprop_decay= 0.99\n",
        "rmsprop_constant = 1e-6\n",
        "#\n",
        "\n",
        "n_steps = 4000000\n",
        "training_start = 10000\n",
        "training_interval = 4\n",
        "save_steps = 1000\n",
        "# changed copy_steps from 10000 to 2500\n",
        "copy_steps = 2500\n",
        "discount_rate = 0.99\n",
        "skip_start = 90\n",
        "batch_size = 50\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/breakout_run9.ckpt\"\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    iteration = 0\n",
        "    loss_val = np.infty\n",
        "    game_length = 0\n",
        "    total_max_q = 0\n",
        "    mean_max_q = 0.0\n",
        "    done = True\n",
        "    state = []\n",
        "    final_mean_max_q=[]\n",
        "\n",
        "    dqn = DQN()\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "\n",
        "    X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
        "\n",
        "    online_q_values, online_vars = dqn.create_model(X_state, \"qnetwork_online\")\n",
        "    target_q_values, target_vars = dqn.create_model(X_state, \"qnetwork_target\")\n",
        "\n",
        "    copy_ops = [target_var.assign(online_vars[var_name])\n",
        "                for var_name, target_var in target_vars.items()]\n",
        "    copy_online_to_target = tf.group(*copy_ops)\n",
        "\n",
        "    X_action, global_step, loss, training_op, y = define_train_variables(online_q_values)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        restore_session(copy_online_to_target, init, saver, sess)\n",
        "\n",
        "        while True:\n",
        "            step = global_step.eval()\n",
        "            if step >= n_steps:\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "            print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
        "                iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
        "            \n",
        "\n",
        "\n",
        "            state = skip_some_steps(done, env, state)\n",
        "\n",
        "            done, q_values, next_state = evaluate_and_play_online_dqn(X_state, env, online_q_values, state, step)\n",
        "            state = next_state\n",
        "\n",
        "            mean_max_q = compute_statistics(done, game_length, mean_max_q, q_values, total_max_q)\n",
        "\n",
        "            if done:\n",
        "                # once an episode is done, save the results to a list\n",
        "                final_mean_max_q.append(mean_max_q)\n",
        "\n",
        "            if iteration < training_start or iteration % training_interval != 0:\n",
        "                continue\n",
        "\n",
        "            loss_val = train_online_dqn(X_action, X_state, loss, sess, target_q_values, training_op, y)\n",
        "\n",
        "            # Copy the online DQN to the target DQN\n",
        "            if step % copy_steps == 0:\n",
        "                copy_online_to_target.run()\n",
        "\n",
        "            # Save model\n",
        "            if step % save_steps == 0:\n",
        "                saver.save(sess, checkpoint_path)\n",
        "            \n",
        "            # save mean max q\n",
        "            if step % save_steps == 0:\n",
        "                with open('/content/gdrive/MyDrive/breakout_run9_mean_max_q.txt', 'w') as file:              \n",
        "                  file.write('%s\\n' % final_mean_max_q)\n",
        "\t\t\t\t            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def define_train_variables(online_q_values):\n",
        "    with tf.variable_scope(\"train\"):\n",
        "        X_action = tf.placeholder(tf.int32, shape=[None])\n",
        "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
        "                                axis=1, keepdims=True)\n",
        "        error = tf.abs(y - q_value)\n",
        "        clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
        "        linear_error = 2 * (error - clipped_error)\n",
        "        loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
        "\n",
        "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=rmsprop_decay, momentum=0.0, epsilon=rmsprop_constant, name='RMSProp')\n",
        "        training_op = optimizer.minimize(loss, global_step=global_step)\n",
        "    return X_action, global_step, loss, training_op, y\n",
        "\n",
        "\n",
        "def restore_session(copy_online_to_target, init, saver, sess):\n",
        "    if os.path.isfile(checkpoint_path + \".index\"):\n",
        "        saver.restore(sess, checkpoint_path)\n",
        "        print(\"restored session\")\n",
        "    else:\n",
        "        init.run()\n",
        "        copy_online_to_target.run()\n",
        "        print(\"created a new session\")\n",
        "\n",
        "\n",
        "def skip_some_steps(done, env, state):\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        for skip in range(skip_start):\n",
        "            obs, reward, done, info = env.step(0)\n",
        "        state = preprocess_observation(obs)\n",
        "    return state\n",
        "\n",
        "\n",
        "def evaluate_and_play_online_dqn(X_state, env, online_q_values, state, step):\n",
        "    # evaluate what to do\n",
        "    q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
        "    action = epsilon_greedy(q_values, step)\n",
        "\n",
        "    # play the game\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = preprocess_observation(obs)\n",
        "\n",
        "    # memorize whats happened\n",
        "    replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
        "\n",
        "    return done, q_values, next_state\n",
        "\n",
        "\n",
        "def compute_statistics(done, game_length, mean_max_q, q_values, total_max_q):\n",
        "    total_max_q += q_values.max()\n",
        "    game_length += 1\n",
        "    if done:\n",
        "        mean_max_q = total_max_q / game_length\n",
        "    return mean_max_q\n",
        "\n",
        "\n",
        "def train_online_dqn(X_action, X_state, loss, sess, target_q_values, training_op, y):\n",
        "    # Sample memories and use the target DQN to produce the target Q-Value\n",
        "    X_state_val, X_action_val, rewards, X_next_state_val, continues = (sample_memories(batch_size))\n",
        "    next_q_values = target_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
        "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
        "    y_val = rewards + continues * discount_rate * max_next_q_values\n",
        "\n",
        "    # Train the online DQN\n",
        "    _, loss_val = sess.run([training_op, loss], feed_dict={X_state: X_state_val,\n",
        "                                                           X_action: X_action_val,\n",
        "                                                           y: y_val})\n",
        "    return loss_val\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_model()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-6-4107ab60b92b>:28: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-6-4107ab60b92b>:32: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "created a new session\n",
            "Iteration 1843552\tTraining step 458388/4000000 (11.5)%\tLoss 0.027399\tMean Max-Q 0.672976   "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b474ae0c4ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-b474ae0c4ed4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_online_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# Copy the online DQN to the target DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b474ae0c4ed4>\u001b[0m in \u001b[0;36mtrain_online_dqn\u001b[0;34m(X_action, X_state, loss, sess, target_q_values, training_op, y)\u001b[0m\n\u001b[1;32m    168\u001b[0m     _, loss_val = sess.run([training_op, loss], feed_dict={X_state: X_state_val,\n\u001b[1;32m    169\u001b[0m                                                            \u001b[0mX_action\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_action_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                                                            y: y_val})\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--437xtEMv2G"
      },
      "source": [
        "env = gym.make(\"Breakout-v0\")\n",
        "obs = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-GyxucNuVs"
      },
      "source": [
        "env.unwrapped.get_action_meanings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0mtkjOCodAi"
      },
      "source": [
        "# Agent play\n",
        "# let the trained model play and record the rewards\n",
        "tf.reset_default_graph()\n",
        "number_of_games = 5\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/breakout_run8.ckpt\"\n",
        "\n",
        "input_height = 80\n",
        "input_width = 80\n",
        "input_channels = 1\n",
        "n_outputs = 4\n",
        "\n",
        "def test_model(model_path, number_of_games):\n",
        "    dqn = DQN()\n",
        "    env = gym.make(\"Breakout-v0\")\n",
        "\n",
        "    X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\n",
        "    online_q_values, online_vars = dqn.create_model(X_state, \"qnetwork_online\")\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, model_path)\n",
        "\n",
        "        obs = env.reset()\n",
        "\n",
        "        for step in range(number_of_games):\n",
        "          total_reward_per_episode=0\n",
        "\n",
        "          while True:\n",
        "            state = preprocess_observation(obs)\n",
        "\n",
        "            # evaluates what to do\n",
        "            q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
        "            action = np.argmax(q_values)\n",
        "\n",
        "            # plays the game\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            print(obs)\n",
        "\n",
        "            # add reward from this step to the total episode reward\n",
        "            total_reward_per_episode += reward\n",
        "            \n",
        "            time.sleep(0.05)\n",
        "\n",
        "            if done:\n",
        "                print(\"reward for game %d is %d\" %(number_of_games,total_reward_per_episode))\n",
        "                final_reward_list.append(total_reward_per_episode)\n",
        "                total_reward_per_episode = 0\n",
        "                break\n",
        "            \n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_model(checkpoint_path, number_of_games)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jF90btEdCQT"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO3kCadOog4B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}