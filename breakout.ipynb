{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "dl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "breakout.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiedali/colab_temp/blob/main/breakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNuJ5va2dS4-",
        "outputId": "92577e1b-86cb-4138-8241-c4d4ee2dc41f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# imports\n",
        "# Reference: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import gym\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "# choose a GPU card\n",
        "os.environ['CUDA_VISIBLE_DEVICES']=\"2\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQATSPWrdS5D",
        "outputId": "657c0d71-0c96-4f5e-e5cb-80e07d026f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
        "obs = env.reset()\n",
        "obs.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 160, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orP8GCzzlKYp"
      },
      "source": [
        "### Helper Functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTyRMNuYlMST"
      },
      "source": [
        "def huber_loss(error):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH0K700pdS5G"
      },
      "source": [
        "def preprocess(image):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
        "    image = image[35:195] # crop\n",
        "    image = image[::2,::2,0] # downsample by factor of 2\n",
        "    image[image == 144] = 0 # erase background (background type 1)\n",
        "    image[image == 109] = 0 # erase background (background type 2)\n",
        "    image[image != 0] = 1 # everything else just set to 1\n",
        "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga14obrTdS5J"
      },
      "source": [
        "## Define Q network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMHrwObWdS5K"
      },
      "source": [
        "# This is now exactly the same as Ghani's CNN settings\n",
        "# need to adjust to 80**)\n",
        "learning_rate=0.001\n",
        "state_size=[80,80,1]\n",
        "action_size=env.action_space.n\n",
        "n_outputs=2 # we are only using action 2 and 3 (RIGHT and LEFT)\n",
        "#\n",
        "input_height=80\n",
        "input_width=80\n",
        "input_channels=1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeAHxexWm738"
      },
      "source": [
        "action_size=env.action_space.n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq9J7MHMm85f",
        "outputId": "5f9d2ddb-6c81-4104-d00c-4448d4ec5799",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "action_size"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwhAMaqxdS5N"
      },
      "source": [
        "# Define a Q network with input size of [80,80], and an output size of size of action space 2\n",
        "# Note that the action space is 4 with following meanings:\n",
        "# ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
        "# We will only be taking action 2 or 3\n",
        "class DQN(object):\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='online_q_network'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.name =name\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            with tf.name_scope(\"inputs\"):\n",
        "                # Jieda note: state is preprocessd 80*80*1 array\n",
        "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
        "\n",
        "            with tf.name_scope(\"conv1\"):\n",
        "                self.conv1 = tf.layers.conv2d(\n",
        "                inputs=self.inputs, filters=32, kernel_size=[8, 8], strides=4,\n",
        "                kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "                padding=\"VALID\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
        "                \n",
        "                self.conv1_out = tf.nn.relu(self.conv1, name='conv1_out')\n",
        "             \n",
        "            with tf.name_scope(\"conv2\"):\n",
        "                \n",
        "                self.conv2 = tf.layers.conv2d(\n",
        "                inputs = self.conv1_out, filters=64,\n",
        "                kernel_size=[4,4], strides=[2,2],padding=\"VALID\",\n",
        "                kernel_initializer = tf.variance_scaling_initializer(scale=2),\n",
        "                activation=tf.nn.relu, use_bias=False, name='conv2')\n",
        "                \n",
        "                self.conv2_out = tf.nn.relu(self.conv2, name='conv2_out')\n",
        "            \n",
        "            with tf.name_scope(\"conv3\"):\n",
        "            \n",
        "                self.conv3 = tf.layers.conv2d(\n",
        "                inputs = self.conv2_out, filters=64,\n",
        "                 kernel_size=[3,3], strides=[1,1], padding=\"VALID\",\n",
        "                 kernel_initializer = tf.variance_scaling_initializer(scale=2),\n",
        "                 name = \"conv3\")\n",
        "                \n",
        "                self.conv3_out = tf.nn.relu(self.conv3, name='conv3_out')\n",
        "            \n",
        "            with tf.name_scope(\"flatten\"):\n",
        "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "                \n",
        "\n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
        "                                          units = 512, activation = tf.nn.relu,\n",
        "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
        "            \n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc2 = tf.layers.dense(inputs=self.flatten,\n",
        "                                          units = 512, activation = tf.nn.relu,\n",
        "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc2\")\n",
        "\n",
        "            with tf.name_scope(\"outputs\"):\n",
        "                self.outputs = tf.layers.dense(inputs = self.fc2,\n",
        "                                              units = action_size,\n",
        "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                              activation = None)\n",
        "            # Output is the approximated Action Values Q(s,a), so we don't need any activation \n",
        "\n",
        "    def get_outputs(self):\n",
        "      \n",
        "        return self.outputs\n",
        "\n",
        "    def get_weights(self,scope_name):\n",
        "        # give all the weights of that network\n",
        "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name)\n",
        "        # create a dictionary to contain the values of the network weights\n",
        "        trainable_vars_by_name = {var.name[len(scope_name):]: var\n",
        "                                for var in trainable_vars}\n",
        "        return trainable_vars_by_name  \n",
        "#             with tf.name_scope(\"loss\"):\n",
        "#                 self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits, labels =self.actions)\n",
        "#                 # Jieda noted: for baseline, we subtract the value_estimate_ from discounted_episode_rewards\n",
        "#                 self.weighted_negative_likelihoods = tf.multiply(self.cross_entropy, self.discounted_episode_rewards)\n",
        "#                 self.loss = tf.reduce_mean(self.weighted_negative_likelihoods)\n",
        "\n",
        "#             with tf.name_scope(\"train\"):\n",
        "#                 self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
        "#                 self.train_opt = self.optimizer.minimize(self.loss)\n",
        "tf.reset_default_graph()\n",
        "X_state=tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
        "# initialize the two Q network\n",
        "online_q = DQN(state_size, action_size, learning_rate, 'online_q_network')\n",
        "target_q = DQN(state_size, action_size, learning_rate, 'target_q_network')\n",
        "# get the output and weights from online q network\n",
        "online_q_values=online_q.get_outputs()\n",
        "online_q_weights=online_q.get_weights(scope_name='online_q_network')\n",
        "# # get the output and weights from target q network\n",
        "target_q_values=target_q.get_outputs()\n",
        "target_q_weights=target_q.get_weights(scope_name='target_q_network')\n",
        "# define the operation to copy the online network weights to target_q_network weights\n",
        "copy_ops = [target_var.assign(online_q_weights[var_name]) for var_name, target_var in target_q_weights.items()]\n",
        "#\n",
        "copy_online_to_target = tf.group(*copy_ops)\n",
        "\n",
        "## define the train operation\n",
        "with tf.variable_scope(\"train\"):\n",
        "  # define training operation\n",
        "  input_action = tf.placeholder(tf.int32, shape=[None])\n",
        "  y=tf.placeholder(tf.float32, shape=[None, 1])\n",
        "  # Get the Q values for the input_action\n",
        "  q_value = tf.reduce_sum(online_q_values*tf.one_hot(input_action,action_size),axis=1,keepdims=True)\n",
        "  # compute the error between lable y and q_value from online_q_network approximation\n",
        "  # Note that lable y is computed using the target_q_network\n",
        "  error=tf.abs(y-q_value)\n",
        "  clipped_error = tf.clip_by_value(error,0,1)\n",
        "  linear_error = 2*(error-clipped_error)\n",
        "  loss = tf.reduce_mean(tf.square(clipped_error)+linear_error)\n",
        "\n",
        "  # global_step is used to keep track of number of training steps completed\n",
        "  global_step = tf.Variable(0,trainable=False,name='global_step')\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
        "  training_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "init=tf.global_variables_initializer()\n",
        "saver=tf.train.Saver()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgbRi7PiB-X",
        "outputId": "76c9ecff-50d6-4e2d-cb6a-1af0107f8ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "target_q_weights.items()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('/conv1/kernel:0', <tf.Variable 'target_q_network/conv1/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>), ('/conv2/kernel:0', <tf.Variable 'target_q_network/conv2/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>), ('/conv3/kernel:0', <tf.Variable 'target_q_network/conv3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>), ('/conv3/bias:0', <tf.Variable 'target_q_network/conv3/bias:0' shape=(64,) dtype=float32_ref>), ('/fc1/kernel:0', <tf.Variable 'target_q_network/fc1/kernel:0' shape=(2304, 512) dtype=float32_ref>), ('/fc1/bias:0', <tf.Variable 'target_q_network/fc1/bias:0' shape=(512,) dtype=float32_ref>), ('/fc2/kernel:0', <tf.Variable 'target_q_network/fc2/kernel:0' shape=(2304, 512) dtype=float32_ref>), ('/fc2/bias:0', <tf.Variable 'target_q_network/fc2/bias:0' shape=(512,) dtype=float32_ref>), ('/dense/kernel:0', <tf.Variable 'target_q_network/dense/kernel:0' shape=(512, 2) dtype=float32_ref>), ('/dense/bias:0', <tf.Variable 'target_q_network/dense/bias:0' shape=(2,) dtype=float32_ref>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sFNs3dkg9Ke",
        "outputId": "1d389a78-0e43-47de-b2e2-e6a539e30896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "online_q_values"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'online_q_network/outputs/dense/BiasAdd:0' shape=(?, 2) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMc4J_Q0dS5Q"
      },
      "source": [
        "## Learning parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNIBQOBgdS5T"
      },
      "source": [
        "learning_rate=0.001\n",
        "# Use adam optimizer\n",
        "beta_1=0.9\n",
        "beta_2=0.999\n",
        "epsilon=1e-07"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1i2_PnudS5V"
      },
      "source": [
        "## Replay Buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXcon2vgdS5Y"
      },
      "source": [
        "replay_memory_size=500000\n",
        "replay_memory = deque([],maxlen=replay_memory_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2cFiXRzdS5a"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self,maxlen):\n",
        "        self.maxlen=maxlen\n",
        "        self.buf = \n",
        "\n",
        "    def draw_samples_from_rb(batch_size):\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hly4XS-YdS5c"
      },
      "source": [
        "## Epsilon Greedy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jWFJhALdS5f"
      },
      "source": [
        "eps_min = 0.1\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 2000000\n",
        "def epsilon_greedy(q_values, step):\n",
        "    # Note: we gradually decrease epsilon, we explore more in the beginning, less towards later\n",
        "    epsilon = max(eps_min, eps_max-(eps_max-eps_min)*step/eps_decay_steps)\n",
        "    if np.random.rand()<epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return np.argmax(q_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRK1-PLzdS5h"
      },
      "source": [
        "## Training parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ9fPlxudS5j"
      },
      "source": [
        "# This is the number of training steps \n",
        "n_steps = 4000000\n",
        "# In the very beginning, First have 10000 samples in the replay buffer\n",
        "training_start=10000\n",
        "# Before each training step, we run 4 episodes and add those samples to replay buffer\n",
        "training_interval=4\n",
        "# \n",
        "save_steps=1000\n",
        "copy_steps = 10000 # copy the online network parameters to target network every 10000 steps\n",
        "discount_rate=0.99\n",
        "skip_start=90\n",
        "batch_size=50\n",
        "iteration=0\n",
        "done=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfFRPB2HrYVn"
      },
      "source": [
        "with tf.Session as sess:\n",
        "  if os.path.isfile(checkpoint_path+\".index\"):\n",
        "    saver.restore(sess, checkpoint_path)\n",
        "  else:\n",
        "    init.run()\n",
        "    # to make sure they have the same values to begin with\n",
        "    # otherwise, different seeds will result in different initialized network weights\n",
        "    copy_online_to_target.run()\n",
        "  # start training\n",
        "  # Training loop : \n",
        "  # (1) collect M data points, add to replay buffer (play 4 episodes of game, add those to replay buffer)\n",
        "  # (2) copy online network parameter to target network\n",
        "  # (3) sample a batch from replay buffer, do the online network update (batch size:50)\n",
        "  while True:\n",
        "    step = global_step.eval()\n",
        "    # if global training steps have meet the maximum, we will stop training\n",
        "    if step >= n_steps:\n",
        "      break\n",
        "    # One iteration is just trakcing ONE sample generation !!!\n",
        "    iteration+=1\n",
        "    print(\"Iteration %d \\t Training step %d/%d loss value %f \\t Mean Max Q %f\" %(iteration, step, n_step, loss_val, mean_max_q))\n",
        "\n",
        "    if done:\n",
        "      obs = env.reset()\n",
        "      for skip in range(skip_start): # skip the beginning of each game\n",
        "        obs, reward, done, info = env.step(0) # take action 0\n",
        "      state = preprocess(obs)\n",
        "    \n",
        "    # online network evaluates what to do\n",
        "    q_values = online_q_values.eval(feed_dict={X_state:[state])})\n",
        "    # q_values returns array of 4 values corresponding to 4 actions\n",
        "    # for each step in the game, we record the max of q_values \n",
        "    action = epsilon_greedy(q_values)\n",
        "\n",
        "    # online network plays\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = preprocess(obs)\n",
        "\n",
        "    # Add this sample to the replay buffer\n",
        "    replay_buffer.append((state, action, reward, next_state, 1.0-done))\n",
        "    state=next_state\n",
        "\n",
        "    # compute statistics to track training progress\n",
        "    total_max_q +=q_values.max()\n",
        "    game_length +=1\n",
        "    if done:\n",
        "      mean_max_q = total_max_q/game_length\n",
        "      total_max_q=0.0\n",
        "      game_length = 0\n",
        "    \n",
        "    if iteration < training_start or iteration % training_interval !=0:\n",
        "      continue\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}