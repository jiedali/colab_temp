{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "dl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "breakout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiedali/colab_temp/blob/main/breakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNuJ5va2dS4-",
        "outputId": "345d9f82-fbd1-4a9f-c589-19d877a3837e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# imports\n",
        "# Reference: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import gym\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "# choose a GPU card\n",
        "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
        "# Set seed for tensorflow\n",
        "SEED=123\n",
        "tf.set_random_seed(SEED)\n",
        "GYM_SEED=678\n"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQATSPWrdS5D",
        "outputId": "4d06e163-9ff3-4b5e-8e81-6071ac3ab871",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make(\"Breakout-v0\")\n",
        "obs = env.reset()\n",
        "env.seed(GYM_SEED)\n",
        "obs.shape"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 160, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orP8GCzzlKYp"
      },
      "source": [
        "### Helper Functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH0K700pdS5G"
      },
      "source": [
        "def preprocess(image):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
        "    image = image[35:195] # crop\n",
        "    image = image[::2,::2,0] # downsample by factor of 2\n",
        "    image[image == 144] = 0 # erase background (background type 1)\n",
        "    image[image == 109] = 0 # erase background (background type 2)\n",
        "    image[image != 0] = 1 # everything else just set to 1\n",
        "    return np.reshape(image.astype(np.float).ravel(), [80,80])"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga14obrTdS5J"
      },
      "source": [
        "## Define Q network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMHrwObWdS5K"
      },
      "source": [
        "# This is now exactly the same as Ghani's CNN settings\n",
        "# need to adjust to 80**)\n",
        "learning_rate=0.001\n",
        "state_size=[80,80,1]\n",
        "action_size=env.action_space.n\n",
        "n_outputs=action_size \n",
        "input_height=80\n",
        "input_width=80\n",
        "input_channels=1"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwhAMaqxdS5N"
      },
      "source": [
        "# Define a Q network with input size of [80,80], and an output size of size of action space 2\n",
        "# Note that the action space is 4 with following meanings:\n",
        "# ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
        "# We will only be taking action 2 or 3\n",
        "class DQN(object):\n",
        "    def __init__(self, state_size, action_size, learning_rate, name='online_q_network'):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.name =name\n",
        "        \n",
        "        with tf.variable_scope(name):\n",
        "            with tf.name_scope(\"inputs\"):\n",
        "                # Jieda note: state is preprocessd 80*80*1 array\n",
        "                self.inputs = tf.placeholder(tf.float32, [None,*state_size], name = \"inputs\")\n",
        "\n",
        "            with tf.name_scope(\"conv1\"):\n",
        "                self.conv1 = tf.layers.conv2d(\n",
        "                inputs=self.inputs, filters=32, kernel_size=[8, 8], strides=4,\n",
        "                kernel_initializer=tf.variance_scaling_initializer(scale=2),\n",
        "                padding=\"VALID\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
        "                \n",
        "                self.conv1_out = tf.nn.relu(self.conv1, name='conv1_out')\n",
        "             \n",
        "            with tf.name_scope(\"conv2\"):\n",
        "                \n",
        "                self.conv2 = tf.layers.conv2d(\n",
        "                inputs = self.conv1_out, filters=64,\n",
        "                kernel_size=[4,4], strides=[2,2],padding=\"VALID\",\n",
        "                kernel_initializer = tf.variance_scaling_initializer(scale=2),\n",
        "                activation=tf.nn.relu, use_bias=False, name='conv2')\n",
        "                \n",
        "                self.conv2_out = tf.nn.relu(self.conv2, name='conv2_out')\n",
        "            \n",
        "            with tf.name_scope(\"conv3\"):\n",
        "            \n",
        "                self.conv3 = tf.layers.conv2d(\n",
        "                inputs = self.conv2_out, filters=64,\n",
        "                 kernel_size=[3,3], strides=[1,1], padding=\"VALID\",\n",
        "                 kernel_initializer = tf.variance_scaling_initializer(scale=2),\n",
        "                 name = \"conv3\")\n",
        "                \n",
        "                self.conv3_out = tf.nn.relu(self.conv3, name='conv3_out')\n",
        "            \n",
        "            with tf.name_scope(\"flatten\"):\n",
        "                self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
        "                \n",
        "\n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc1 = tf.layers.dense(inputs=self.flatten,\n",
        "                                          units = 512, activation = tf.nn.relu,\n",
        "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc1\")\n",
        "            \n",
        "            with tf.name_scope(\"fc1\"):\n",
        "                self.fc2 = tf.layers.dense(inputs=self.flatten,\n",
        "                                          units = 512, activation = tf.nn.relu,\n",
        "                                          kernel_initializer = tf.contrib.layers.xavier_initializer(), name = \"fc2\")\n",
        "\n",
        "            with tf.name_scope(\"outputs\"):\n",
        "                self.outputs = tf.layers.dense(inputs = self.fc2,\n",
        "                                              units = action_size,\n",
        "                                              kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
        "                                              activation = None)\n",
        "            # Output is the approximated Action Values Q(s,a), so we don't need any activation \n",
        "\n",
        "    def get_outputs(self):\n",
        "      \n",
        "        return self.outputs\n",
        "\n",
        "    def get_weights(self,scope_name):\n",
        "        # give all the weights of that network\n",
        "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name)\n",
        "        # create a dictionary to contain the values of the network weights\n",
        "        trainable_vars_by_name = {var.name[len(scope_name):]: var\n",
        "                                for var in trainable_vars}\n",
        "        return trainable_vars_by_name  \n",
        "\n",
        "#\n",
        "tf.reset_default_graph()\n",
        "X_state=tf.placeholder(tf.float32,shape=[None,input_height,input_width,input_channels])\n",
        "# initialize the two Q network\n",
        "online_q = DQN(state_size, action_size, learning_rate, 'online_q_network')\n",
        "target_q = DQN(state_size, action_size, learning_rate, 'target_q_network')\n",
        "# get the output and weights from online q network\n",
        "online_q_values=online_q.get_outputs()\n",
        "online_q_weights=online_q.get_weights(scope_name='online_q_network')\n",
        "# # get the output and weights from target q network\n",
        "target_q_values=target_q.get_outputs()\n",
        "target_q_weights=target_q.get_weights(scope_name='target_q_network')\n",
        "# define the operation to copy the online network weights to target_q_network weights\n",
        "copy_ops = [target_var.assign(online_q_weights[var_name]) for var_name, target_var in target_q_weights.items()]\n",
        "#\n",
        "copy_online_to_target = tf.group(*copy_ops)\n",
        "\n",
        "## define the train operation\n",
        "with tf.variable_scope(\"train\"):\n",
        "  # define training operation\n",
        "  input_action = tf.placeholder(tf.int32, shape=[None])\n",
        "  y=tf.placeholder(tf.float32, shape=[None, 1])\n",
        "  # Get the Q values for the input_action\n",
        "  q_value = tf.reduce_sum(online_q_values*tf.one_hot(input_action,action_size),axis=1,keepdims=True)\n",
        "  # compute the error between lable y and q_value from online_q_network approximation\n",
        "  # Note that lable y is computed using the target_q_network\n",
        "  error=tf.abs(y-q_value)\n",
        "  clipped_error = tf.clip_by_value(error,0,1)\n",
        "  linear_error = 2*(error-clipped_error)\n",
        "  loss = tf.reduce_mean(tf.square(clipped_error)+linear_error)\n",
        "\n",
        "  # global_step is used to keep track of number of training steps completed\n",
        "  global_step = tf.Variable(0,trainable=False,name='global_step')\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-08,use_locking=False,name='Adam')\n",
        "  training_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "init=tf.global_variables_initializer()\n",
        "saver=tf.train.Saver()"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgbRi7PiB-X",
        "outputId": "f451e077-c2dd-48c5-ba4d-85b6866f681e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "target_q_weights.items()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('/conv1/kernel:0', <tf.Variable 'target_q_network/conv1/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>), ('/conv2/kernel:0', <tf.Variable 'target_q_network/conv2/kernel:0' shape=(4, 4, 32, 64) dtype=float32_ref>), ('/conv3/kernel:0', <tf.Variable 'target_q_network/conv3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>), ('/conv3/bias:0', <tf.Variable 'target_q_network/conv3/bias:0' shape=(64,) dtype=float32_ref>), ('/fc1/kernel:0', <tf.Variable 'target_q_network/fc1/kernel:0' shape=(2304, 512) dtype=float32_ref>), ('/fc1/bias:0', <tf.Variable 'target_q_network/fc1/bias:0' shape=(512,) dtype=float32_ref>), ('/fc2/kernel:0', <tf.Variable 'target_q_network/fc2/kernel:0' shape=(2304, 512) dtype=float32_ref>), ('/fc2/bias:0', <tf.Variable 'target_q_network/fc2/bias:0' shape=(512,) dtype=float32_ref>), ('/dense/kernel:0', <tf.Variable 'target_q_network/dense/kernel:0' shape=(512, 4) dtype=float32_ref>), ('/dense/bias:0', <tf.Variable 'target_q_network/dense/bias:0' shape=(4,) dtype=float32_ref>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sFNs3dkg9Ke",
        "outputId": "129e15c1-4a85-451a-9af8-a3dab1c8fc7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "online_q_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'online_q_network/outputs/dense/BiasAdd:0' shape=(?, 4) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMc4J_Q0dS5Q"
      },
      "source": [
        "## Learning parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNIBQOBgdS5T"
      },
      "source": [
        "learning_rate=0.001\n",
        "# Use adam optimizer\n",
        "beta_1=0.9\n",
        "beta_2=0.999\n",
        "epsilon=1e-07"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXcon2vgdS5Y"
      },
      "source": [
        "replay_memory_size=500000\n",
        "# replay_memory = deque([],maxlen=replay_memory_size)\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self,maxlen):\n",
        "        self.maxlen=maxlen\n",
        "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
        "        self.index=0\n",
        "        self.length=0\n",
        "\n",
        "    def append(self, data):\n",
        "        self.buf[self.index] = data\n",
        "        self.length = min(self.length+1, self.maxlen)\n",
        "        self.index = (self.index + 1) % self.maxlen\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # sample without replacement\n",
        "        indices = np.random.permutation(self.length)[:batch_size]\n",
        "        return self.buf[indices]\n",
        "\n",
        "# create an instance of ReplayBuffer\n",
        "replay_buffer = ReplayBuffer(replay_memory_size)\n",
        "def sample_replay_buffer(batch_size):\n",
        "  cols =[[],[],[],[],[]] # state, action, reward, next_state, continue\n",
        "  for memory in replay_buffer.sample(batch_size):\n",
        "    for col, value in zip(cols, memory):\n",
        "      col.append(value)\n",
        "  cols = [np.array(col) for col in cols]\n",
        "  return cols[0], cols[1], cols[2].reshape(-1,1), cols[3], cols[4].reshape(-1,1)"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ryCMNiMuJSt",
        "outputId": "0d496d32-4fd7-49e3-c67d-c54b73381e5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "replay_buffer = ReplayBuffer(replay_memory_size)\n",
        "memory = replay_buffer.sample(2)\n",
        "memory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hly4XS-YdS5c"
      },
      "source": [
        "## Epsilon Greedy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jWFJhALdS5f"
      },
      "source": [
        "eps_min = 0.1\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 2000000\n",
        "def epsilon_greedy(q_values, step):\n",
        "    # Note: we gradually decrease epsilon, we explore more in the beginning, less towards later\n",
        "    epsilon = max(eps_min, eps_max-(eps_max-eps_min)*step/eps_decay_steps)\n",
        "    if np.random.rand()<epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return np.argmax(q_values)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRK1-PLzdS5h"
      },
      "source": [
        "## Training parameters"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ9fPlxudS5j"
      },
      "source": [
        "# This is the number of training steps \n",
        "n_steps = 4000000\n",
        "# In the very beginning, First have 10000 samples in the replay buffer\n",
        "training_start=10000\n",
        "# Before each training step, we run 4 episodes and add those samples to replay buffer\n",
        "training_interval=4\n",
        "# \n",
        "save_steps=1000\n",
        "copy_steps = 2500 # copy the online network parameters to target network every 10000 steps\n",
        "discount_rate=0.99\n",
        "skip_start=90\n",
        "batch_size=50\n",
        "iteration=0\n",
        "done=True\n",
        "# initialize the tracking statistics\n",
        "max_episode_reward_so_far=0\n",
        "total_max_q=-float('inf')\n",
        "mean_reward=0\n",
        "total_reward=0\n",
        "game_length=0\n",
        "game_counter=0"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oWN5YJKvfkO",
        "outputId": "f5ea3b62-8f20-40c5-c92f-734fbd9cbe91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "obs, reward, done, info = env.step(0)\n",
        "state = preprocess(obs)\n",
        "state=state.reshape(80,80,1)\n",
        "state.shape"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 80, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AwaUFrkzLzI",
        "outputId": "3150def2-4670-496e-b501-431fd5ae24a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result=sample_replay_buffer(10)\n",
        "result[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfFRPB2HrYVn",
        "outputId": "d59714fb-5c52-4e82-fa5f-0506e20094d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # if os.path.isfile(checkpoint_path +\".index\"):\n",
        "  #   saver.restore(sess, checkpoint_path)\n",
        "  # else:\n",
        "  init.run()\n",
        "  # to make sure they have the same values to begin with\n",
        "  # otherwise, different seeds will result in different initialized network weights\n",
        "  copy_online_to_target.run()\n",
        "  # intialize replay buffer\n",
        "  replay_buffer = ReplayBuffer(replay_memory_size)\n",
        "  # start training\n",
        "  # Training loop : \n",
        "  # (1) collect M data points, add to replay buffer (play 4 episodes of game, add those to replay buffer)\n",
        "  # (2) copy online network parameter to target network\n",
        "  # (3) sample a batch from replay buffer, do the online network update (batch size:50)\n",
        "  while True:\n",
        "    step = global_step.eval()\n",
        "    # if global training steps have meet the maximum, we will stop training\n",
        "    if step >= n_steps:\n",
        "      break\n",
        "    # One iteration is just one sample generation !!!\n",
        "    iteration+=1\n",
        "    if done:\n",
        "      obs = env.reset()\n",
        "      for skip in range(skip_start): # skip the beginning of each game\n",
        "        obs, reward, done, info = env.step(0) # take action 0\n",
        "      state = preprocess(obs)\n",
        "    state = state.reshape(1,80,80,1)\n",
        "    state_to_rb=state.reshape(80,80,1)\n",
        "      \n",
        "    # online network evaluates what to do\n",
        "    q_values = online_q_values.eval(feed_dict={online_q.inputs:state})\n",
        "    # q_values returns array of 4 values corresponding to 4 actions\n",
        "    # for each step in the game, we record the max of q_values \n",
        "    action = epsilon_greedy(q_values,step)\n",
        "\n",
        "    # online network plays\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    next_state = preprocess(obs)\n",
        "    next_state = next_state.reshape(1,80,80,1)\n",
        "    next_state_to_rb = next_state.reshape(80,80,1)\n",
        "\n",
        "    # Add this sample to the replay buffer\n",
        "    replay_buffer.append((state_to_rb, action, reward, next_state_to_rb, 1.0-done))\n",
        "    # pass \"next_state\" to \"state\"\n",
        "    state=next_state\n",
        "    state_to_rb=next_state_to_rb\n",
        "\n",
        "    # compute statistics to track training progress\n",
        "    # track the maximum episode reward obtained so far, and the average episode reward\n",
        "    total_reward += reward\n",
        "    total_max_q += q_values.max()\n",
        "    game_length +=1\n",
        "    if done:\n",
        "      game_counter +=1\n",
        "      mean_max_q = total_max_q/game_length\n",
        "      mean_reward = total_reward/game_length\n",
        "      if total_reward > max_episode_reward_so_far:\n",
        "        max_episode_reward_so_far = total_reward\n",
        "      total_max_q=0.0\n",
        "      game_length = 0\n",
        "      total_reward=0\n",
        "\n",
        "    # if number of samples are less than training_start(10000), or it is not a multiple of 4, we will NOT do training\n",
        "    if iteration < training_start or iteration % training_interval !=0:\n",
        "      continue\n",
        "    \n",
        "    if iteration % 100 == 0: # print stats every 100 training steps\n",
        "      print(\"Iteration %d\\tTraining step %d/%d\\t Mean Max Q %f\\tepisode total reward %d\\t Max episode reward so far %d\\t Game length %d\\t Game counts %d\" %(iteration, step, n_steps, mean_max_q, total_reward, max_episode_reward_so_far, game_length, game_counter))\n",
        "    # otherwise, if iteration is more than 10,000 and it is a multiple of 4, we will update the network\n",
        "    # result=sample_replay_buffer(batch_size)\n",
        "    X_state_val, X_action_val, rewards, X_next_state_val, continues = sample_replay_buffer(batch_size)\n",
        "    #\n",
        "    next_q_values = target_q_values.eval(\n",
        "        feed_dict = {target_q.inputs: X_next_state_val}\n",
        "    )\n",
        "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
        "    y_val = rewards + continues*discount_rate*max_next_q_values\n",
        "\n",
        "    # train online q network\n",
        "    _, loss_val = sess.run([training_op,loss], feed_dict={online_q.inputs:X_state_val, input_action: X_action_val, y:y_val})\n",
        "\n",
        "    # copy online q network to target network\n",
        "    if step % copy_steps ==0:\n",
        "      copy_online_to_target.run()\n",
        "    \n",
        "    # save dqn regularly\n",
        "    # if step % save_steps ==0:\n",
        "    #   saver.save(sess,checkpoint_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 10100\tTraining step 24/4000000\t Mean Max Q 0.253977\tepisode total reward 3\t Max episode reward so far 6\t Game length 275\t Game counts 40\n",
            "Iteration 10200\tTraining step 49/4000000\t Mean Max Q 0.253977\tepisode total reward 4\t Max episode reward so far 6\t Game length 375\t Game counts 40\n",
            "Iteration 10300\tTraining step 74/4000000\t Mean Max Q 3.607826\tepisode total reward 1\t Max episode reward so far 6\t Game length 69\t Game counts 41\n",
            "Iteration 10400\tTraining step 99/4000000\t Mean Max Q 3.607826\tepisode total reward 2\t Max episode reward so far 6\t Game length 169\t Game counts 41\n",
            "Iteration 10500\tTraining step 124/4000000\t Mean Max Q 3.607826\tepisode total reward 4\t Max episode reward so far 6\t Game length 269\t Game counts 41\n",
            "Iteration 10600\tTraining step 149/4000000\t Mean Max Q 3.607826\tepisode total reward 5\t Max episode reward so far 6\t Game length 369\t Game counts 41\n",
            "Iteration 10700\tTraining step 174/4000000\t Mean Max Q 5.810148\tepisode total reward 0\t Max episode reward so far 6\t Game length 38\t Game counts 42\n",
            "Iteration 10800\tTraining step 199/4000000\t Mean Max Q 5.810148\tepisode total reward 1\t Max episode reward so far 6\t Game length 138\t Game counts 42\n",
            "Iteration 10900\tTraining step 224/4000000\t Mean Max Q 5.810148\tepisode total reward 2\t Max episode reward so far 6\t Game length 238\t Game counts 42\n",
            "Iteration 11000\tTraining step 249/4000000\t Mean Max Q 5.902443\tepisode total reward 0\t Max episode reward so far 6\t Game length 39\t Game counts 43\n",
            "Iteration 11100\tTraining step 274/4000000\t Mean Max Q 5.902443\tepisode total reward 0\t Max episode reward so far 6\t Game length 139\t Game counts 43\n",
            "Iteration 11200\tTraining step 299/4000000\t Mean Max Q 5.968976\tepisode total reward 1\t Max episode reward so far 6\t Game length 52\t Game counts 44\n",
            "Iteration 11300\tTraining step 324/4000000\t Mean Max Q 5.968976\tepisode total reward 1\t Max episode reward so far 6\t Game length 152\t Game counts 44\n",
            "Iteration 11400\tTraining step 349/4000000\t Mean Max Q 5.968976\tepisode total reward 3\t Max episode reward so far 6\t Game length 252\t Game counts 44\n",
            "Iteration 11500\tTraining step 374/4000000\t Mean Max Q 5.922991\tepisode total reward 0\t Max episode reward so far 6\t Game length 48\t Game counts 45\n",
            "Iteration 11600\tTraining step 399/4000000\t Mean Max Q 5.922991\tepisode total reward 0\t Max episode reward so far 6\t Game length 148\t Game counts 45\n",
            "Iteration 11700\tTraining step 424/4000000\t Mean Max Q 5.975702\tepisode total reward 0\t Max episode reward so far 6\t Game length 71\t Game counts 46\n",
            "Iteration 11800\tTraining step 449/4000000\t Mean Max Q 5.975702\tepisode total reward 0\t Max episode reward so far 6\t Game length 171\t Game counts 46\n",
            "Iteration 11900\tTraining step 474/4000000\t Mean Max Q 5.959768\tepisode total reward 0\t Max episode reward so far 6\t Game length 61\t Game counts 47\n",
            "Iteration 12000\tTraining step 499/4000000\t Mean Max Q 5.959768\tepisode total reward 0\t Max episode reward so far 6\t Game length 161\t Game counts 47\n",
            "Iteration 12100\tTraining step 524/4000000\t Mean Max Q 5.955621\tepisode total reward 0\t Max episode reward so far 6\t Game length 90\t Game counts 48\n",
            "Iteration 12200\tTraining step 549/4000000\t Mean Max Q 5.955621\tepisode total reward 1\t Max episode reward so far 6\t Game length 190\t Game counts 48\n",
            "Iteration 12300\tTraining step 574/4000000\t Mean Max Q 5.944608\tepisode total reward 0\t Max episode reward so far 6\t Game length 34\t Game counts 49\n",
            "Iteration 12400\tTraining step 599/4000000\t Mean Max Q 5.944608\tepisode total reward 0\t Max episode reward so far 6\t Game length 134\t Game counts 49\n",
            "Iteration 12500\tTraining step 624/4000000\t Mean Max Q 5.949010\tepisode total reward 0\t Max episode reward so far 6\t Game length 55\t Game counts 50\n",
            "Iteration 12600\tTraining step 649/4000000\t Mean Max Q 5.949010\tepisode total reward 0\t Max episode reward so far 6\t Game length 155\t Game counts 50\n",
            "Iteration 12700\tTraining step 674/4000000\t Mean Max Q 5.949010\tepisode total reward 2\t Max episode reward so far 6\t Game length 255\t Game counts 50\n",
            "Iteration 12800\tTraining step 699/4000000\t Mean Max Q 5.972168\tepisode total reward 0\t Max episode reward so far 6\t Game length 81\t Game counts 51\n",
            "Iteration 12900\tTraining step 724/4000000\t Mean Max Q 5.972168\tepisode total reward 2\t Max episode reward so far 6\t Game length 181\t Game counts 51\n",
            "Iteration 13000\tTraining step 749/4000000\t Mean Max Q 5.972168\tepisode total reward 2\t Max episode reward so far 6\t Game length 281\t Game counts 51\n",
            "Iteration 13100\tTraining step 774/4000000\t Mean Max Q 5.915275\tepisode total reward 0\t Max episode reward so far 6\t Game length 85\t Game counts 52\n",
            "Iteration 13200\tTraining step 799/4000000\t Mean Max Q 5.915275\tepisode total reward 0\t Max episode reward so far 6\t Game length 185\t Game counts 52\n",
            "Iteration 13300\tTraining step 824/4000000\t Mean Max Q 5.915275\tepisode total reward 2\t Max episode reward so far 6\t Game length 285\t Game counts 52\n",
            "Iteration 13400\tTraining step 849/4000000\t Mean Max Q 5.949865\tepisode total reward 0\t Max episode reward so far 6\t Game length 95\t Game counts 53\n",
            "Iteration 13500\tTraining step 874/4000000\t Mean Max Q 5.938866\tepisode total reward 0\t Max episode reward so far 6\t Game length 2\t Game counts 54\n",
            "Iteration 13600\tTraining step 899/4000000\t Mean Max Q 5.938866\tepisode total reward 0\t Max episode reward so far 6\t Game length 102\t Game counts 54\n",
            "Iteration 13700\tTraining step 924/4000000\t Mean Max Q 5.952103\tepisode total reward 0\t Max episode reward so far 6\t Game length 29\t Game counts 55\n",
            "Iteration 13800\tTraining step 949/4000000\t Mean Max Q 5.952103\tepisode total reward 0\t Max episode reward so far 6\t Game length 129\t Game counts 55\n",
            "Iteration 13900\tTraining step 974/4000000\t Mean Max Q 5.952103\tepisode total reward 1\t Max episode reward so far 6\t Game length 229\t Game counts 55\n",
            "Iteration 14000\tTraining step 999/4000000\t Mean Max Q 5.948508\tepisode total reward 0\t Max episode reward so far 6\t Game length 15\t Game counts 56\n",
            "Iteration 14100\tTraining step 1024/4000000\t Mean Max Q 5.948508\tepisode total reward 1\t Max episode reward so far 6\t Game length 115\t Game counts 56\n",
            "Iteration 14200\tTraining step 1049/4000000\t Mean Max Q 5.948508\tepisode total reward 1\t Max episode reward so far 6\t Game length 215\t Game counts 56\n",
            "Iteration 14300\tTraining step 1074/4000000\t Mean Max Q 5.948508\tepisode total reward 3\t Max episode reward so far 6\t Game length 315\t Game counts 56\n",
            "Iteration 14400\tTraining step 1099/4000000\t Mean Max Q 5.876337\tepisode total reward 0\t Max episode reward so far 6\t Game length 43\t Game counts 57\n",
            "Iteration 14500\tTraining step 1124/4000000\t Mean Max Q 5.876337\tepisode total reward 0\t Max episode reward so far 6\t Game length 143\t Game counts 57\n",
            "Iteration 14600\tTraining step 1149/4000000\t Mean Max Q 5.951089\tepisode total reward 0\t Max episode reward so far 6\t Game length 63\t Game counts 58\n",
            "Iteration 14700\tTraining step 1174/4000000\t Mean Max Q 5.951089\tepisode total reward 1\t Max episode reward so far 6\t Game length 163\t Game counts 58\n",
            "Iteration 14800\tTraining step 1199/4000000\t Mean Max Q 5.972579\tepisode total reward 0\t Max episode reward so far 6\t Game length 54\t Game counts 59\n",
            "Iteration 14900\tTraining step 1224/4000000\t Mean Max Q 5.972579\tepisode total reward 0\t Max episode reward so far 6\t Game length 154\t Game counts 59\n",
            "Iteration 15000\tTraining step 1249/4000000\t Mean Max Q 5.972579\tepisode total reward 2\t Max episode reward so far 6\t Game length 254\t Game counts 59\n",
            "Iteration 15100\tTraining step 1274/4000000\t Mean Max Q 5.960760\tepisode total reward 1\t Max episode reward so far 6\t Game length 79\t Game counts 60\n",
            "Iteration 15200\tTraining step 1299/4000000\t Mean Max Q 5.960760\tepisode total reward 1\t Max episode reward so far 6\t Game length 179\t Game counts 60\n",
            "Iteration 15300\tTraining step 1324/4000000\t Mean Max Q 5.922119\tepisode total reward 0\t Max episode reward so far 6\t Game length 1\t Game counts 61\n",
            "Iteration 15400\tTraining step 1349/4000000\t Mean Max Q 5.922119\tepisode total reward 1\t Max episode reward so far 6\t Game length 101\t Game counts 61\n",
            "Iteration 15500\tTraining step 1374/4000000\t Mean Max Q 5.922119\tepisode total reward 1\t Max episode reward so far 6\t Game length 201\t Game counts 61\n",
            "Iteration 15600\tTraining step 1399/4000000\t Mean Max Q 5.922119\tepisode total reward 2\t Max episode reward so far 6\t Game length 301\t Game counts 61\n",
            "Iteration 15700\tTraining step 1424/4000000\t Mean Max Q 5.939167\tepisode total reward 1\t Max episode reward so far 6\t Game length 92\t Game counts 62\n",
            "Iteration 15800\tTraining step 1449/4000000\t Mean Max Q 5.939167\tepisode total reward 3\t Max episode reward so far 6\t Game length 192\t Game counts 62\n",
            "Iteration 15900\tTraining step 1474/4000000\t Mean Max Q 5.939167\tepisode total reward 3\t Max episode reward so far 6\t Game length 292\t Game counts 62\n",
            "Iteration 16000\tTraining step 1499/4000000\t Mean Max Q 5.939167\tepisode total reward 4\t Max episode reward so far 6\t Game length 392\t Game counts 62\n",
            "Iteration 16100\tTraining step 1524/4000000\t Mean Max Q 5.855859\tepisode total reward 0\t Max episode reward so far 6\t Game length 63\t Game counts 63\n",
            "Iteration 16200\tTraining step 1549/4000000\t Mean Max Q 5.855859\tepisode total reward 1\t Max episode reward so far 6\t Game length 163\t Game counts 63\n",
            "Iteration 16300\tTraining step 1574/4000000\t Mean Max Q 5.855859\tepisode total reward 2\t Max episode reward so far 6\t Game length 263\t Game counts 63\n",
            "Iteration 16400\tTraining step 1599/4000000\t Mean Max Q 5.957929\tepisode total reward 0\t Max episode reward so far 6\t Game length 91\t Game counts 64\n",
            "Iteration 16500\tTraining step 1624/4000000\t Mean Max Q 5.938334\tepisode total reward 0\t Max episode reward so far 6\t Game length 1\t Game counts 65\n",
            "Iteration 16600\tTraining step 1649/4000000\t Mean Max Q 5.938334\tepisode total reward 1\t Max episode reward so far 6\t Game length 101\t Game counts 65\n",
            "Iteration 16700\tTraining step 1674/4000000\t Mean Max Q 5.938334\tepisode total reward 1\t Max episode reward so far 6\t Game length 201\t Game counts 65\n",
            "Iteration 16800\tTraining step 1699/4000000\t Mean Max Q 5.936041\tepisode total reward 0\t Max episode reward so far 6\t Game length 69\t Game counts 66\n",
            "Iteration 16900\tTraining step 1724/4000000\t Mean Max Q 5.936041\tepisode total reward 0\t Max episode reward so far 6\t Game length 169\t Game counts 66\n",
            "Iteration 17000\tTraining step 1749/4000000\t Mean Max Q 5.934150\tepisode total reward 1\t Max episode reward so far 6\t Game length 93\t Game counts 67\n",
            "Iteration 17100\tTraining step 1774/4000000\t Mean Max Q 5.934150\tepisode total reward 2\t Max episode reward so far 6\t Game length 193\t Game counts 67\n",
            "Iteration 17200\tTraining step 1799/4000000\t Mean Max Q 5.850066\tepisode total reward 0\t Max episode reward so far 6\t Game length 44\t Game counts 68\n",
            "Iteration 17300\tTraining step 1824/4000000\t Mean Max Q 5.850066\tepisode total reward 1\t Max episode reward so far 6\t Game length 144\t Game counts 68\n",
            "Iteration 17400\tTraining step 1849/4000000\t Mean Max Q 5.850066\tepisode total reward 2\t Max episode reward so far 6\t Game length 244\t Game counts 68\n",
            "Iteration 17500\tTraining step 1874/4000000\t Mean Max Q 5.850066\tepisode total reward 3\t Max episode reward so far 6\t Game length 344\t Game counts 68\n",
            "Iteration 17600\tTraining step 1899/4000000\t Mean Max Q 5.918573\tepisode total reward 1\t Max episode reward so far 6\t Game length 84\t Game counts 69\n",
            "Iteration 17700\tTraining step 1924/4000000\t Mean Max Q 5.918573\tepisode total reward 2\t Max episode reward so far 6\t Game length 184\t Game counts 69\n",
            "Iteration 17800\tTraining step 1949/4000000\t Mean Max Q 5.918573\tepisode total reward 2\t Max episode reward so far 6\t Game length 284\t Game counts 69\n",
            "Iteration 17900\tTraining step 1974/4000000\t Mean Max Q 5.860821\tepisode total reward 0\t Max episode reward so far 6\t Game length 84\t Game counts 70\n",
            "Iteration 18000\tTraining step 1999/4000000\t Mean Max Q 5.860821\tepisode total reward 1\t Max episode reward so far 6\t Game length 184\t Game counts 70\n",
            "Iteration 18100\tTraining step 2024/4000000\t Mean Max Q 5.860821\tepisode total reward 3\t Max episode reward so far 6\t Game length 284\t Game counts 70\n",
            "Iteration 18200\tTraining step 2049/4000000\t Mean Max Q 5.860821\tepisode total reward 4\t Max episode reward so far 6\t Game length 384\t Game counts 70\n",
            "Iteration 18300\tTraining step 2074/4000000\t Mean Max Q 5.851403\tepisode total reward 0\t Max episode reward so far 6\t Game length 75\t Game counts 71\n",
            "Iteration 18400\tTraining step 2099/4000000\t Mean Max Q 5.851403\tepisode total reward 0\t Max episode reward so far 6\t Game length 175\t Game counts 71\n",
            "Iteration 18500\tTraining step 2124/4000000\t Mean Max Q 5.926655\tepisode total reward 0\t Max episode reward so far 6\t Game length 93\t Game counts 72\n",
            "Iteration 18600\tTraining step 2149/4000000\t Mean Max Q 5.934691\tepisode total reward 0\t Max episode reward so far 6\t Game length 23\t Game counts 73\n",
            "Iteration 18700\tTraining step 2174/4000000\t Mean Max Q 5.934691\tepisode total reward 0\t Max episode reward so far 6\t Game length 123\t Game counts 73\n",
            "Iteration 18800\tTraining step 2199/4000000\t Mean Max Q 5.928725\tepisode total reward 0\t Max episode reward so far 6\t Game length 62\t Game counts 74\n",
            "Iteration 18900\tTraining step 2224/4000000\t Mean Max Q 5.928725\tepisode total reward 1\t Max episode reward so far 6\t Game length 162\t Game counts 74\n",
            "Iteration 19000\tTraining step 2249/4000000\t Mean Max Q 5.929447\tepisode total reward 0\t Max episode reward so far 6\t Game length 47\t Game counts 75\n",
            "Iteration 19100\tTraining step 2274/4000000\t Mean Max Q 5.929447\tepisode total reward 0\t Max episode reward so far 6\t Game length 147\t Game counts 75\n",
            "Iteration 19200\tTraining step 2299/4000000\t Mean Max Q 5.929447\tepisode total reward 2\t Max episode reward so far 6\t Game length 247\t Game counts 75\n",
            "Iteration 19300\tTraining step 2324/4000000\t Mean Max Q 5.947786\tepisode total reward 0\t Max episode reward so far 6\t Game length 69\t Game counts 76\n",
            "Iteration 19400\tTraining step 2349/4000000\t Mean Max Q 5.947786\tepisode total reward 1\t Max episode reward so far 6\t Game length 169\t Game counts 76\n",
            "Iteration 19500\tTraining step 2374/4000000\t Mean Max Q 5.947786\tepisode total reward 2\t Max episode reward so far 6\t Game length 269\t Game counts 76\n",
            "Iteration 19600\tTraining step 2399/4000000\t Mean Max Q 5.910132\tepisode total reward 0\t Max episode reward so far 6\t Game length 54\t Game counts 77\n",
            "Iteration 19700\tTraining step 2424/4000000\t Mean Max Q 5.910132\tepisode total reward 0\t Max episode reward so far 6\t Game length 154\t Game counts 77\n",
            "Iteration 19800\tTraining step 2449/4000000\t Mean Max Q 5.929096\tepisode total reward 0\t Max episode reward so far 6\t Game length 71\t Game counts 78\n",
            "Iteration 19900\tTraining step 2474/4000000\t Mean Max Q 5.936211\tepisode total reward 0\t Max episode reward so far 6\t Game length 12\t Game counts 79\n",
            "Iteration 20000\tTraining step 2499/4000000\t Mean Max Q 5.936211\tepisode total reward 0\t Max episode reward so far 6\t Game length 112\t Game counts 79\n",
            "Iteration 20100\tTraining step 2524/4000000\t Mean Max Q 5.907329\tepisode total reward 0\t Max episode reward so far 6\t Game length 39\t Game counts 80\n",
            "Iteration 20200\tTraining step 2549/4000000\t Mean Max Q 5.907329\tepisode total reward 0\t Max episode reward so far 6\t Game length 139\t Game counts 80\n",
            "Iteration 20300\tTraining step 2574/4000000\t Mean Max Q 5.934725\tepisode total reward 1\t Max episode reward so far 6\t Game length 64\t Game counts 81\n",
            "Iteration 20400\tTraining step 2599/4000000\t Mean Max Q 5.934725\tepisode total reward 1\t Max episode reward so far 6\t Game length 164\t Game counts 81\n",
            "Iteration 20500\tTraining step 2624/4000000\t Mean Max Q 5.934725\tepisode total reward 2\t Max episode reward so far 6\t Game length 264\t Game counts 81\n",
            "Iteration 20600\tTraining step 2649/4000000\t Mean Max Q 5.839821\tepisode total reward 0\t Max episode reward so far 6\t Game length 90\t Game counts 82\n",
            "Iteration 20700\tTraining step 2674/4000000\t Mean Max Q 5.934474\tepisode total reward 0\t Max episode reward so far 6\t Game length 23\t Game counts 83\n",
            "Iteration 20800\tTraining step 2699/4000000\t Mean Max Q 5.934474\tepisode total reward 1\t Max episode reward so far 6\t Game length 123\t Game counts 83\n",
            "Iteration 20900\tTraining step 2724/4000000\t Mean Max Q 5.934474\tepisode total reward 2\t Max episode reward so far 6\t Game length 223\t Game counts 83\n",
            "Iteration 21000\tTraining step 2749/4000000\t Mean Max Q 5.841797\tepisode total reward 0\t Max episode reward so far 6\t Game length 10\t Game counts 84\n",
            "Iteration 21100\tTraining step 2774/4000000\t Mean Max Q 5.841797\tepisode total reward 1\t Max episode reward so far 6\t Game length 110\t Game counts 84\n",
            "Iteration 21200\tTraining step 2799/4000000\t Mean Max Q 5.841797\tepisode total reward 1\t Max episode reward so far 6\t Game length 210\t Game counts 84\n",
            "Iteration 21300\tTraining step 2824/4000000\t Mean Max Q 5.917516\tepisode total reward 0\t Max episode reward so far 6\t Game length 4\t Game counts 85\n",
            "Iteration 21400\tTraining step 2849/4000000\t Mean Max Q 5.917516\tepisode total reward 1\t Max episode reward so far 6\t Game length 104\t Game counts 85\n",
            "Iteration 21500\tTraining step 2874/4000000\t Mean Max Q 5.917516\tepisode total reward 3\t Max episode reward so far 6\t Game length 204\t Game counts 85\n",
            "Iteration 21600\tTraining step 2899/4000000\t Mean Max Q 5.917516\tepisode total reward 3\t Max episode reward so far 6\t Game length 304\t Game counts 85\n",
            "Iteration 21700\tTraining step 2924/4000000\t Mean Max Q 5.837739\tepisode total reward 0\t Max episode reward so far 6\t Game length 64\t Game counts 86\n",
            "Iteration 21800\tTraining step 2949/4000000\t Mean Max Q 5.837739\tepisode total reward 1\t Max episode reward so far 6\t Game length 164\t Game counts 86\n",
            "Iteration 21900\tTraining step 2974/4000000\t Mean Max Q 5.921915\tepisode total reward 0\t Max episode reward so far 6\t Game length 14\t Game counts 87\n",
            "Iteration 22000\tTraining step 2999/4000000\t Mean Max Q 5.921915\tepisode total reward 0\t Max episode reward so far 6\t Game length 114\t Game counts 87\n",
            "Iteration 22100\tTraining step 3024/4000000\t Mean Max Q 5.940688\tepisode total reward 0\t Max episode reward so far 6\t Game length 42\t Game counts 88\n",
            "Iteration 22200\tTraining step 3049/4000000\t Mean Max Q 5.940688\tepisode total reward 1\t Max episode reward so far 6\t Game length 142\t Game counts 88\n",
            "Iteration 22300\tTraining step 3074/4000000\t Mean Max Q 5.940688\tepisode total reward 2\t Max episode reward so far 6\t Game length 242\t Game counts 88\n",
            "Iteration 22400\tTraining step 3099/4000000\t Mean Max Q 5.893898\tepisode total reward 0\t Max episode reward so far 6\t Game length 43\t Game counts 89\n",
            "Iteration 22500\tTraining step 3124/4000000\t Mean Max Q 5.893898\tepisode total reward 0\t Max episode reward so far 6\t Game length 143\t Game counts 89\n",
            "Iteration 22600\tTraining step 3149/4000000\t Mean Max Q 5.928881\tepisode total reward 0\t Max episode reward so far 6\t Game length 56\t Game counts 90\n",
            "Iteration 22700\tTraining step 3174/4000000\t Mean Max Q 5.928881\tepisode total reward 0\t Max episode reward so far 6\t Game length 156\t Game counts 90\n",
            "Iteration 22800\tTraining step 3199/4000000\t Mean Max Q 5.919713\tepisode total reward 0\t Max episode reward so far 6\t Game length 80\t Game counts 91\n",
            "Iteration 22900\tTraining step 3224/4000000\t Mean Max Q 5.919713\tepisode total reward 0\t Max episode reward so far 6\t Game length 180\t Game counts 91\n",
            "Iteration 23000\tTraining step 3249/4000000\t Mean Max Q 5.926547\tepisode total reward 0\t Max episode reward so far 6\t Game length 39\t Game counts 92\n",
            "Iteration 23100\tTraining step 3274/4000000\t Mean Max Q 5.926547\tepisode total reward 1\t Max episode reward so far 6\t Game length 139\t Game counts 92\n",
            "Iteration 23200\tTraining step 3299/4000000\t Mean Max Q 5.926547\tepisode total reward 2\t Max episode reward so far 6\t Game length 239\t Game counts 92\n",
            "Iteration 23300\tTraining step 3324/4000000\t Mean Max Q 5.921716\tepisode total reward 0\t Max episode reward so far 6\t Game length 61\t Game counts 93\n",
            "Iteration 23400\tTraining step 3349/4000000\t Mean Max Q 5.921716\tepisode total reward 0\t Max episode reward so far 6\t Game length 161\t Game counts 93\n",
            "Iteration 23500\tTraining step 3374/4000000\t Mean Max Q 5.916934\tepisode total reward 0\t Max episode reward so far 6\t Game length 88\t Game counts 94\n",
            "Iteration 23600\tTraining step 3399/4000000\t Mean Max Q 5.916934\tepisode total reward 1\t Max episode reward so far 6\t Game length 188\t Game counts 94\n",
            "Iteration 23700\tTraining step 3424/4000000\t Mean Max Q 5.916934\tepisode total reward 2\t Max episode reward so far 6\t Game length 288\t Game counts 94\n",
            "Iteration 23800\tTraining step 3449/4000000\t Mean Max Q 5.863147\tepisode total reward 0\t Max episode reward so far 6\t Game length 88\t Game counts 95\n",
            "Iteration 23900\tTraining step 3474/4000000\t Mean Max Q 5.900167\tepisode total reward 0\t Max episode reward so far 6\t Game length 19\t Game counts 96\n",
            "Iteration 24000\tTraining step 3499/4000000\t Mean Max Q 5.900167\tepisode total reward 0\t Max episode reward so far 6\t Game length 119\t Game counts 96\n",
            "Iteration 24100\tTraining step 3524/4000000\t Mean Max Q 5.912003\tepisode total reward 0\t Max episode reward so far 6\t Game length 37\t Game counts 97\n",
            "Iteration 24200\tTraining step 3549/4000000\t Mean Max Q 5.912003\tepisode total reward 1\t Max episode reward so far 6\t Game length 137\t Game counts 97\n",
            "Iteration 24300\tTraining step 3574/4000000\t Mean Max Q 5.912003\tepisode total reward 2\t Max episode reward so far 6\t Game length 237\t Game counts 97\n",
            "Iteration 24400\tTraining step 3599/4000000\t Mean Max Q 5.912065\tepisode total reward 0\t Max episode reward so far 6\t Game length 56\t Game counts 98\n",
            "Iteration 24500\tTraining step 3624/4000000\t Mean Max Q 5.912065\tepisode total reward 0\t Max episode reward so far 6\t Game length 156\t Game counts 98\n",
            "Iteration 24600\tTraining step 3649/4000000\t Mean Max Q 5.908067\tepisode total reward 0\t Max episode reward so far 6\t Game length 82\t Game counts 99\n",
            "Iteration 24700\tTraining step 3674/4000000\t Mean Max Q 5.920242\tepisode total reward 0\t Max episode reward so far 6\t Game length 4\t Game counts 100\n",
            "Iteration 24800\tTraining step 3699/4000000\t Mean Max Q 5.920242\tepisode total reward 0\t Max episode reward so far 6\t Game length 104\t Game counts 100\n",
            "Iteration 24900\tTraining step 3724/4000000\t Mean Max Q 5.920242\tepisode total reward 2\t Max episode reward so far 6\t Game length 204\t Game counts 100\n",
            "Iteration 25000\tTraining step 3749/4000000\t Mean Max Q 5.831507\tepisode total reward 0\t Max episode reward so far 6\t Game length 53\t Game counts 101\n",
            "Iteration 25100\tTraining step 3774/4000000\t Mean Max Q 5.831507\tepisode total reward 0\t Max episode reward so far 6\t Game length 153\t Game counts 101\n",
            "Iteration 25200\tTraining step 3799/4000000\t Mean Max Q 5.831507\tepisode total reward 3\t Max episode reward so far 6\t Game length 253\t Game counts 101\n",
            "Iteration 25300\tTraining step 3824/4000000\t Mean Max Q 5.831507\tepisode total reward 4\t Max episode reward so far 6\t Game length 353\t Game counts 101\n",
            "Iteration 25400\tTraining step 3849/4000000\t Mean Max Q 5.900659\tepisode total reward 0\t Max episode reward so far 6\t Game length 95\t Game counts 102\n",
            "Iteration 25500\tTraining step 3874/4000000\t Mean Max Q 5.929732\tepisode total reward 0\t Max episode reward so far 6\t Game length 32\t Game counts 103\n",
            "Iteration 25600\tTraining step 3899/4000000\t Mean Max Q 5.929732\tepisode total reward 1\t Max episode reward so far 6\t Game length 132\t Game counts 103\n",
            "Iteration 25700\tTraining step 3924/4000000\t Mean Max Q 5.929732\tepisode total reward 1\t Max episode reward so far 6\t Game length 232\t Game counts 103\n",
            "Iteration 25800\tTraining step 3949/4000000\t Mean Max Q 5.930824\tepisode total reward 1\t Max episode reward so far 6\t Game length 89\t Game counts 104\n",
            "Iteration 25900\tTraining step 3974/4000000\t Mean Max Q 5.930824\tepisode total reward 1\t Max episode reward so far 6\t Game length 189\t Game counts 104\n",
            "Iteration 26000\tTraining step 3999/4000000\t Mean Max Q 5.930824\tepisode total reward 2\t Max episode reward so far 6\t Game length 289\t Game counts 104\n",
            "Iteration 26100\tTraining step 4024/4000000\t Mean Max Q 5.847504\tepisode total reward 0\t Max episode reward so far 6\t Game length 29\t Game counts 105\n",
            "Iteration 26200\tTraining step 4049/4000000\t Mean Max Q 5.847504\tepisode total reward 0\t Max episode reward so far 6\t Game length 129\t Game counts 105\n",
            "Iteration 26300\tTraining step 4074/4000000\t Mean Max Q 5.847504\tepisode total reward 1\t Max episode reward so far 6\t Game length 229\t Game counts 105\n",
            "Iteration 26400\tTraining step 4099/4000000\t Mean Max Q 5.916998\tepisode total reward 0\t Max episode reward so far 6\t Game length 91\t Game counts 106\n",
            "Iteration 26500\tTraining step 4124/4000000\t Mean Max Q 5.916998\tepisode total reward 1\t Max episode reward so far 6\t Game length 191\t Game counts 106\n",
            "Iteration 26600\tTraining step 4149/4000000\t Mean Max Q 5.916998\tepisode total reward 2\t Max episode reward so far 6\t Game length 291\t Game counts 106\n",
            "Iteration 26700\tTraining step 4174/4000000\t Mean Max Q 5.879108\tepisode total reward 0\t Max episode reward so far 6\t Game length 92\t Game counts 107\n",
            "Iteration 26800\tTraining step 4199/4000000\t Mean Max Q 5.921502\tepisode total reward 0\t Max episode reward so far 6\t Game length 8\t Game counts 108\n",
            "Iteration 26900\tTraining step 4224/4000000\t Mean Max Q 5.921502\tepisode total reward 0\t Max episode reward so far 6\t Game length 108\t Game counts 108\n",
            "Iteration 27000\tTraining step 4249/4000000\t Mean Max Q 5.921235\tepisode total reward 0\t Max episode reward so far 6\t Game length 44\t Game counts 109\n",
            "Iteration 27100\tTraining step 4274/4000000\t Mean Max Q 5.921235\tepisode total reward 0\t Max episode reward so far 6\t Game length 144\t Game counts 109\n",
            "Iteration 27200\tTraining step 4299/4000000\t Mean Max Q 5.921235\tepisode total reward 2\t Max episode reward so far 6\t Game length 244\t Game counts 109\n",
            "Iteration 27300\tTraining step 4324/4000000\t Mean Max Q 5.923245\tepisode total reward 0\t Max episode reward so far 6\t Game length 80\t Game counts 110\n",
            "Iteration 27400\tTraining step 4349/4000000\t Mean Max Q 5.923245\tepisode total reward 1\t Max episode reward so far 6\t Game length 180\t Game counts 110\n",
            "Iteration 27500\tTraining step 4374/4000000\t Mean Max Q 5.877249\tepisode total reward 0\t Max episode reward so far 6\t Game length 41\t Game counts 111\n",
            "Iteration 27600\tTraining step 4399/4000000\t Mean Max Q 5.877249\tepisode total reward 0\t Max episode reward so far 6\t Game length 141\t Game counts 111\n",
            "Iteration 27700\tTraining step 4424/4000000\t Mean Max Q 5.877249\tepisode total reward 2\t Max episode reward so far 6\t Game length 241\t Game counts 111\n",
            "Iteration 27800\tTraining step 4449/4000000\t Mean Max Q 5.917442\tepisode total reward 0\t Max episode reward so far 6\t Game length 55\t Game counts 112\n",
            "Iteration 27900\tTraining step 4474/4000000\t Mean Max Q 5.917442\tepisode total reward 1\t Max episode reward so far 6\t Game length 155\t Game counts 112\n",
            "Iteration 28000\tTraining step 4499/4000000\t Mean Max Q 5.897537\tepisode total reward 0\t Max episode reward so far 6\t Game length 14\t Game counts 113\n",
            "Iteration 28100\tTraining step 4524/4000000\t Mean Max Q 5.897537\tepisode total reward 0\t Max episode reward so far 6\t Game length 114\t Game counts 113\n",
            "Iteration 28200\tTraining step 4549/4000000\t Mean Max Q 5.924442\tepisode total reward 0\t Max episode reward so far 6\t Game length 45\t Game counts 114\n",
            "Iteration 28300\tTraining step 4574/4000000\t Mean Max Q 5.924442\tepisode total reward 1\t Max episode reward so far 6\t Game length 145\t Game counts 114\n",
            "Iteration 28400\tTraining step 4599/4000000\t Mean Max Q 5.882130\tepisode total reward 0\t Max episode reward so far 6\t Game length 13\t Game counts 115\n",
            "Iteration 28500\tTraining step 4624/4000000\t Mean Max Q 5.882130\tepisode total reward 1\t Max episode reward so far 6\t Game length 113\t Game counts 115\n",
            "Iteration 28600\tTraining step 4649/4000000\t Mean Max Q 5.882130\tepisode total reward 1\t Max episode reward so far 6\t Game length 213\t Game counts 115\n",
            "Iteration 28700\tTraining step 4674/4000000\t Mean Max Q 5.913875\tepisode total reward 0\t Max episode reward so far 6\t Game length 38\t Game counts 116\n",
            "Iteration 28800\tTraining step 4699/4000000\t Mean Max Q 5.913875\tepisode total reward 1\t Max episode reward so far 6\t Game length 138\t Game counts 116\n",
            "Iteration 28900\tTraining step 4724/4000000\t Mean Max Q 5.927166\tepisode total reward 0\t Max episode reward so far 6\t Game length 6\t Game counts 117\n",
            "Iteration 29000\tTraining step 4749/4000000\t Mean Max Q 5.927166\tepisode total reward 0\t Max episode reward so far 6\t Game length 106\t Game counts 117\n",
            "Iteration 29100\tTraining step 4774/4000000\t Mean Max Q 5.922209\tepisode total reward 0\t Max episode reward so far 6\t Game length 32\t Game counts 118\n",
            "Iteration 29200\tTraining step 4799/4000000\t Mean Max Q 5.922209\tepisode total reward 0\t Max episode reward so far 6\t Game length 132\t Game counts 118\n",
            "Iteration 29300\tTraining step 4824/4000000\t Mean Max Q 5.922799\tepisode total reward 0\t Max episode reward so far 6\t Game length 39\t Game counts 119\n",
            "Iteration 29400\tTraining step 4849/4000000\t Mean Max Q 5.922799\tepisode total reward 1\t Max episode reward so far 6\t Game length 139\t Game counts 119\n",
            "Iteration 29500\tTraining step 4874/4000000\t Mean Max Q 5.919002\tepisode total reward 0\t Max episode reward so far 6\t Game length 0\t Game counts 120\n",
            "Iteration 29600\tTraining step 4899/4000000\t Mean Max Q 5.919002\tepisode total reward 0\t Max episode reward so far 6\t Game length 100\t Game counts 120\n",
            "Iteration 29700\tTraining step 4924/4000000\t Mean Max Q 5.934162\tepisode total reward 0\t Max episode reward so far 6\t Game length 34\t Game counts 121\n",
            "Iteration 29800\tTraining step 4949/4000000\t Mean Max Q 5.934162\tepisode total reward 1\t Max episode reward so far 6\t Game length 134\t Game counts 121\n",
            "Iteration 29900\tTraining step 4974/4000000\t Mean Max Q 5.934162\tepisode total reward 1\t Max episode reward so far 6\t Game length 234\t Game counts 121\n",
            "Iteration 30000\tTraining step 4999/4000000\t Mean Max Q 5.873712\tepisode total reward 0\t Max episode reward so far 6\t Game length 86\t Game counts 122\n",
            "Iteration 30100\tTraining step 5024/4000000\t Mean Max Q 5.863604\tepisode total reward 0\t Max episode reward so far 6\t Game length 16\t Game counts 123\n",
            "Iteration 30200\tTraining step 5049/4000000\t Mean Max Q 5.863604\tepisode total reward 2\t Max episode reward so far 6\t Game length 116\t Game counts 123\n",
            "Iteration 30300\tTraining step 5074/4000000\t Mean Max Q 5.863604\tepisode total reward 2\t Max episode reward so far 6\t Game length 216\t Game counts 123\n",
            "Iteration 30400\tTraining step 5099/4000000\t Mean Max Q 5.742001\tepisode total reward 0\t Max episode reward so far 6\t Game length 61\t Game counts 124\n",
            "Iteration 30500\tTraining step 5124/4000000\t Mean Max Q 5.742001\tepisode total reward 1\t Max episode reward so far 6\t Game length 161\t Game counts 124\n",
            "Iteration 30600\tTraining step 5149/4000000\t Mean Max Q 5.742001\tepisode total reward 3\t Max episode reward so far 6\t Game length 261\t Game counts 124\n",
            "Iteration 30700\tTraining step 5174/4000000\t Mean Max Q 5.742001\tepisode total reward 4\t Max episode reward so far 6\t Game length 361\t Game counts 124\n",
            "Iteration 30800\tTraining step 5199/4000000\t Mean Max Q 5.772609\tepisode total reward 0\t Max episode reward so far 6\t Game length 35\t Game counts 125\n",
            "Iteration 30900\tTraining step 5224/4000000\t Mean Max Q 5.772609\tepisode total reward 1\t Max episode reward so far 6\t Game length 135\t Game counts 125\n",
            "Iteration 31000\tTraining step 5249/4000000\t Mean Max Q 5.772609\tepisode total reward 2\t Max episode reward so far 6\t Game length 235\t Game counts 125\n",
            "Iteration 31100\tTraining step 5274/4000000\t Mean Max Q 5.813187\tepisode total reward 0\t Max episode reward so far 6\t Game length 20\t Game counts 126\n",
            "Iteration 31200\tTraining step 5299/4000000\t Mean Max Q 5.813187\tepisode total reward 1\t Max episode reward so far 6\t Game length 120\t Game counts 126\n",
            "Iteration 31300\tTraining step 5324/4000000\t Mean Max Q 5.813187\tepisode total reward 2\t Max episode reward so far 6\t Game length 220\t Game counts 126\n",
            "Iteration 31400\tTraining step 5349/4000000\t Mean Max Q 5.812358\tepisode total reward 0\t Max episode reward so far 6\t Game length 52\t Game counts 127\n",
            "Iteration 31500\tTraining step 5374/4000000\t Mean Max Q 5.812358\tepisode total reward 0\t Max episode reward so far 6\t Game length 152\t Game counts 127\n",
            "Iteration 31600\tTraining step 5399/4000000\t Mean Max Q 5.812429\tepisode total reward 0\t Max episode reward so far 6\t Game length 69\t Game counts 128\n",
            "Iteration 31700\tTraining step 5424/4000000\t Mean Max Q 5.812429\tepisode total reward 0\t Max episode reward so far 6\t Game length 169\t Game counts 128\n",
            "Iteration 31800\tTraining step 5449/4000000\t Mean Max Q 5.819110\tepisode total reward 0\t Max episode reward so far 6\t Game length 30\t Game counts 129\n",
            "Iteration 31900\tTraining step 5474/4000000\t Mean Max Q 5.819110\tepisode total reward 0\t Max episode reward so far 6\t Game length 130\t Game counts 129\n",
            "Iteration 32000\tTraining step 5499/4000000\t Mean Max Q 5.797507\tepisode total reward 0\t Max episode reward so far 6\t Game length 61\t Game counts 130\n",
            "Iteration 32100\tTraining step 5524/4000000\t Mean Max Q 5.797507\tepisode total reward 1\t Max episode reward so far 6\t Game length 161\t Game counts 130\n",
            "Iteration 32200\tTraining step 5549/4000000\t Mean Max Q 5.797507\tepisode total reward 2\t Max episode reward so far 6\t Game length 261\t Game counts 130\n",
            "Iteration 32300\tTraining step 5574/4000000\t Mean Max Q 5.813654\tepisode total reward 1\t Max episode reward so far 6\t Game length 85\t Game counts 131\n",
            "Iteration 32400\tTraining step 5599/4000000\t Mean Max Q 5.813654\tepisode total reward 1\t Max episode reward so far 6\t Game length 185\t Game counts 131\n",
            "Iteration 32500\tTraining step 5624/4000000\t Mean Max Q 5.726667\tepisode total reward 0\t Max episode reward so far 6\t Game length 75\t Game counts 132\n",
            "Iteration 32600\tTraining step 5649/4000000\t Mean Max Q 5.726667\tepisode total reward 0\t Max episode reward so far 6\t Game length 175\t Game counts 132\n",
            "Iteration 32700\tTraining step 5674/4000000\t Mean Max Q 5.802104\tepisode total reward 0\t Max episode reward so far 6\t Game length 30\t Game counts 133\n",
            "Iteration 32800\tTraining step 5699/4000000\t Mean Max Q 5.802104\tepisode total reward 0\t Max episode reward so far 6\t Game length 130\t Game counts 133\n",
            "Iteration 32900\tTraining step 5724/4000000\t Mean Max Q 5.808040\tepisode total reward 0\t Max episode reward so far 6\t Game length 8\t Game counts 134\n",
            "Iteration 33000\tTraining step 5749/4000000\t Mean Max Q 5.808040\tepisode total reward 0\t Max episode reward so far 6\t Game length 108\t Game counts 134\n",
            "Iteration 33100\tTraining step 5774/4000000\t Mean Max Q 5.830502\tepisode total reward 0\t Max episode reward so far 6\t Game length 38\t Game counts 135\n",
            "Iteration 33200\tTraining step 5799/4000000\t Mean Max Q 5.830502\tepisode total reward 0\t Max episode reward so far 6\t Game length 138\t Game counts 135\n",
            "Iteration 33300\tTraining step 5824/4000000\t Mean Max Q 5.815217\tepisode total reward 0\t Max episode reward so far 6\t Game length 57\t Game counts 136\n",
            "Iteration 33400\tTraining step 5849/4000000\t Mean Max Q 5.815217\tepisode total reward 0\t Max episode reward so far 6\t Game length 157\t Game counts 136\n",
            "Iteration 33500\tTraining step 5874/4000000\t Mean Max Q 5.815217\tepisode total reward 2\t Max episode reward so far 6\t Game length 257\t Game counts 136\n",
            "Iteration 33600\tTraining step 5899/4000000\t Mean Max Q 5.816735\tepisode total reward 0\t Max episode reward so far 6\t Game length 76\t Game counts 137\n",
            "Iteration 33700\tTraining step 5924/4000000\t Mean Max Q 5.789758\tepisode total reward 0\t Max episode reward so far 6\t Game length 0\t Game counts 138\n",
            "Iteration 33800\tTraining step 5949/4000000\t Mean Max Q 5.789758\tepisode total reward 0\t Max episode reward so far 6\t Game length 100\t Game counts 138\n",
            "Iteration 33900\tTraining step 5974/4000000\t Mean Max Q 5.789758\tepisode total reward 2\t Max episode reward so far 6\t Game length 200\t Game counts 138\n",
            "Iteration 34000\tTraining step 5999/4000000\t Mean Max Q 5.796615\tepisode total reward 0\t Max episode reward so far 6\t Game length 26\t Game counts 139\n",
            "Iteration 34100\tTraining step 6024/4000000\t Mean Max Q 5.796615\tepisode total reward 0\t Max episode reward so far 6\t Game length 126\t Game counts 139\n",
            "Iteration 34200\tTraining step 6049/4000000\t Mean Max Q 5.799420\tepisode total reward 0\t Max episode reward so far 6\t Game length 49\t Game counts 140\n",
            "Iteration 34300\tTraining step 6074/4000000\t Mean Max Q 5.799420\tepisode total reward 0\t Max episode reward so far 6\t Game length 149\t Game counts 140\n",
            "Iteration 34400\tTraining step 6099/4000000\t Mean Max Q 5.797224\tepisode total reward 0\t Max episode reward so far 6\t Game length 64\t Game counts 141\n",
            "Iteration 34500\tTraining step 6124/4000000\t Mean Max Q 5.797224\tepisode total reward 2\t Max episode reward so far 6\t Game length 164\t Game counts 141\n",
            "Iteration 34600\tTraining step 6149/4000000\t Mean Max Q 5.797224\tepisode total reward 3\t Max episode reward so far 6\t Game length 264\t Game counts 141\n",
            "Iteration 34700\tTraining step 6174/4000000\t Mean Max Q 5.798087\tepisode total reward 0\t Max episode reward so far 6\t Game length 55\t Game counts 142\n",
            "Iteration 34800\tTraining step 6199/4000000\t Mean Max Q 5.798087\tepisode total reward 0\t Max episode reward so far 6\t Game length 155\t Game counts 142\n",
            "Iteration 34900\tTraining step 6224/4000000\t Mean Max Q 5.801852\tepisode total reward 0\t Max episode reward so far 6\t Game length 14\t Game counts 143\n",
            "Iteration 35000\tTraining step 6249/4000000\t Mean Max Q 5.801852\tepisode total reward 0\t Max episode reward so far 6\t Game length 114\t Game counts 143\n",
            "Iteration 35100\tTraining step 6274/4000000\t Mean Max Q 5.801852\tepisode total reward 1\t Max episode reward so far 6\t Game length 214\t Game counts 143\n",
            "Iteration 35200\tTraining step 6299/4000000\t Mean Max Q 5.801852\tepisode total reward 3\t Max episode reward so far 6\t Game length 314\t Game counts 143\n",
            "Iteration 35300\tTraining step 6324/4000000\t Mean Max Q 5.759168\tepisode total reward 0\t Max episode reward so far 6\t Game length 17\t Game counts 144\n",
            "Iteration 35400\tTraining step 6349/4000000\t Mean Max Q 5.759168\tepisode total reward 1\t Max episode reward so far 6\t Game length 117\t Game counts 144\n",
            "Iteration 35500\tTraining step 6374/4000000\t Mean Max Q 5.759168\tepisode total reward 2\t Max episode reward so far 6\t Game length 217\t Game counts 144\n",
            "Iteration 35600\tTraining step 6399/4000000\t Mean Max Q 5.759168\tepisode total reward 3\t Max episode reward so far 6\t Game length 317\t Game counts 144\n",
            "Iteration 35700\tTraining step 6424/4000000\t Mean Max Q 5.759168\tepisode total reward 5\t Max episode reward so far 6\t Game length 417\t Game counts 144\n",
            "Iteration 35800\tTraining step 6449/4000000\t Mean Max Q 5.703222\tepisode total reward 0\t Max episode reward so far 6\t Game length 62\t Game counts 145\n",
            "Iteration 35900\tTraining step 6474/4000000\t Mean Max Q 5.703222\tepisode total reward 1\t Max episode reward so far 6\t Game length 162\t Game counts 145\n",
            "Iteration 36000\tTraining step 6499/4000000\t Mean Max Q 5.810685\tepisode total reward 0\t Max episode reward so far 6\t Game length 61\t Game counts 146\n",
            "Iteration 36100\tTraining step 6524/4000000\t Mean Max Q 5.810685\tepisode total reward 0\t Max episode reward so far 6\t Game length 161\t Game counts 146\n",
            "Iteration 36200\tTraining step 6549/4000000\t Mean Max Q 5.801793\tepisode total reward 0\t Max episode reward so far 6\t Game length 0\t Game counts 147\n",
            "Iteration 36300\tTraining step 6574/4000000\t Mean Max Q 5.801793\tepisode total reward 0\t Max episode reward so far 6\t Game length 100\t Game counts 147\n",
            "Iteration 36400\tTraining step 6599/4000000\t Mean Max Q 5.801793\tepisode total reward 1\t Max episode reward so far 6\t Game length 200\t Game counts 147\n",
            "Iteration 36500\tTraining step 6624/4000000\t Mean Max Q 5.797025\tepisode total reward 0\t Max episode reward so far 6\t Game length 21\t Game counts 148\n",
            "Iteration 36600\tTraining step 6649/4000000\t Mean Max Q 5.797025\tepisode total reward 1\t Max episode reward so far 6\t Game length 121\t Game counts 148\n",
            "Iteration 36700\tTraining step 6674/4000000\t Mean Max Q 5.797025\tepisode total reward 2\t Max episode reward so far 6\t Game length 221\t Game counts 148\n",
            "Iteration 36800\tTraining step 6699/4000000\t Mean Max Q 5.722387\tepisode total reward 0\t Max episode reward so far 6\t Game length 42\t Game counts 149\n",
            "Iteration 36900\tTraining step 6724/4000000\t Mean Max Q 5.722387\tepisode total reward 2\t Max episode reward so far 6\t Game length 142\t Game counts 149\n",
            "Iteration 37000\tTraining step 6749/4000000\t Mean Max Q 5.722387\tepisode total reward 2\t Max episode reward so far 6\t Game length 242\t Game counts 149\n",
            "Iteration 37100\tTraining step 6774/4000000\t Mean Max Q 5.740913\tepisode total reward 1\t Max episode reward so far 6\t Game length 66\t Game counts 150\n",
            "Iteration 37200\tTraining step 6799/4000000\t Mean Max Q 5.740913\tepisode total reward 2\t Max episode reward so far 6\t Game length 166\t Game counts 150\n",
            "Iteration 37300\tTraining step 6824/4000000\t Mean Max Q 5.740913\tepisode total reward 3\t Max episode reward so far 6\t Game length 266\t Game counts 150\n",
            "Iteration 37400\tTraining step 6849/4000000\t Mean Max Q 5.794428\tepisode total reward 0\t Max episode reward so far 6\t Game length 58\t Game counts 151\n",
            "Iteration 37500\tTraining step 6874/4000000\t Mean Max Q 5.794428\tepisode total reward 0\t Max episode reward so far 6\t Game length 158\t Game counts 151\n",
            "Iteration 37600\tTraining step 6899/4000000\t Mean Max Q 5.789629\tepisode total reward 0\t Max episode reward so far 6\t Game length 89\t Game counts 152\n",
            "Iteration 37700\tTraining step 6924/4000000\t Mean Max Q 5.787914\tepisode total reward 0\t Max episode reward so far 6\t Game length 5\t Game counts 153\n",
            "Iteration 37800\tTraining step 6949/4000000\t Mean Max Q 5.787914\tepisode total reward 1\t Max episode reward so far 6\t Game length 105\t Game counts 153\n",
            "Iteration 37900\tTraining step 6974/4000000\t Mean Max Q 5.787914\tepisode total reward 1\t Max episode reward so far 6\t Game length 205\t Game counts 153\n",
            "Iteration 38000\tTraining step 6999/4000000\t Mean Max Q 5.800701\tepisode total reward 0\t Max episode reward so far 6\t Game length 75\t Game counts 154\n",
            "Iteration 38100\tTraining step 7024/4000000\t Mean Max Q 5.800701\tepisode total reward 0\t Max episode reward so far 6\t Game length 175\t Game counts 154\n",
            "Iteration 38200\tTraining step 7049/4000000\t Mean Max Q 5.793041\tepisode total reward 1\t Max episode reward so far 6\t Game length 90\t Game counts 155\n",
            "Iteration 38300\tTraining step 7074/4000000\t Mean Max Q 5.793041\tepisode total reward 2\t Max episode reward so far 6\t Game length 190\t Game counts 155\n",
            "Iteration 38400\tTraining step 7099/4000000\t Mean Max Q 5.776188\tepisode total reward 0\t Max episode reward so far 6\t Game length 0\t Game counts 156\n",
            "Iteration 38500\tTraining step 7124/4000000\t Mean Max Q 5.776188\tepisode total reward 0\t Max episode reward so far 6\t Game length 100\t Game counts 156\n",
            "Iteration 38600\tTraining step 7149/4000000\t Mean Max Q 5.776188\tepisode total reward 1\t Max episode reward so far 6\t Game length 200\t Game counts 156\n",
            "Iteration 38700\tTraining step 7174/4000000\t Mean Max Q 5.794912\tepisode total reward 0\t Max episode reward so far 6\t Game length 23\t Game counts 157\n",
            "Iteration 38800\tTraining step 7199/4000000\t Mean Max Q 5.794912\tepisode total reward 0\t Max episode reward so far 6\t Game length 123\t Game counts 157\n",
            "Iteration 38900\tTraining step 7224/4000000\t Mean Max Q 5.794912\tepisode total reward 2\t Max episode reward so far 6\t Game length 223\t Game counts 157\n",
            "Iteration 39000\tTraining step 7249/4000000\t Mean Max Q 5.800643\tepisode total reward 0\t Max episode reward so far 6\t Game length 54\t Game counts 158\n",
            "Iteration 39100\tTraining step 7274/4000000\t Mean Max Q 5.800643\tepisode total reward 0\t Max episode reward so far 6\t Game length 154\t Game counts 158\n",
            "Iteration 39200\tTraining step 7299/4000000\t Mean Max Q 5.803423\tepisode total reward 0\t Max episode reward so far 6\t Game length 84\t Game counts 159\n",
            "Iteration 39300\tTraining step 7324/4000000\t Mean Max Q 5.803423\tepisode total reward 1\t Max episode reward so far 6\t Game length 184\t Game counts 159\n",
            "Iteration 39400\tTraining step 7349/4000000\t Mean Max Q 5.791413\tepisode total reward 0\t Max episode reward so far 6\t Game length 2\t Game counts 160\n",
            "Iteration 39500\tTraining step 7374/4000000\t Mean Max Q 5.791413\tepisode total reward 0\t Max episode reward so far 6\t Game length 102\t Game counts 160\n",
            "Iteration 39600\tTraining step 7399/4000000\t Mean Max Q 5.791413\tepisode total reward 1\t Max episode reward so far 6\t Game length 202\t Game counts 160\n",
            "Iteration 39700\tTraining step 7424/4000000\t Mean Max Q 5.815816\tepisode total reward 0\t Max episode reward so far 6\t Game length 29\t Game counts 161\n",
            "Iteration 39800\tTraining step 7449/4000000\t Mean Max Q 5.815816\tepisode total reward 1\t Max episode reward so far 6\t Game length 129\t Game counts 161\n",
            "Iteration 39900\tTraining step 7474/4000000\t Mean Max Q 5.756204\tepisode total reward 0\t Max episode reward so far 6\t Game length 29\t Game counts 162\n",
            "Iteration 40000\tTraining step 7499/4000000\t Mean Max Q 5.756204\tepisode total reward 1\t Max episode reward so far 6\t Game length 129\t Game counts 162\n",
            "Iteration 40100\tTraining step 7524/4000000\t Mean Max Q 5.781259\tepisode total reward 0\t Max episode reward so far 6\t Game length 7\t Game counts 163\n",
            "Iteration 40200\tTraining step 7549/4000000\t Mean Max Q 5.781259\tepisode total reward 0\t Max episode reward so far 6\t Game length 107\t Game counts 163\n",
            "Iteration 40300\tTraining step 7574/4000000\t Mean Max Q 5.780999\tepisode total reward 0\t Max episode reward so far 6\t Game length 14\t Game counts 164\n",
            "Iteration 40400\tTraining step 7599/4000000\t Mean Max Q 5.780999\tepisode total reward 0\t Max episode reward so far 6\t Game length 114\t Game counts 164\n",
            "Iteration 40500\tTraining step 7624/4000000\t Mean Max Q 5.780999\tepisode total reward 1\t Max episode reward so far 6\t Game length 214\t Game counts 164\n",
            "Iteration 40600\tTraining step 7649/4000000\t Mean Max Q 5.795024\tepisode total reward 1\t Max episode reward so far 6\t Game length 92\t Game counts 165\n",
            "Iteration 40700\tTraining step 7674/4000000\t Mean Max Q 5.795024\tepisode total reward 2\t Max episode reward so far 6\t Game length 192\t Game counts 165\n",
            "Iteration 40800\tTraining step 7699/4000000\t Mean Max Q 5.795024\tepisode total reward 3\t Max episode reward so far 6\t Game length 292\t Game counts 165\n",
            "Iteration 40900\tTraining step 7724/4000000\t Mean Max Q 5.756591\tepisode total reward 0\t Max episode reward so far 6\t Game length 50\t Game counts 166\n",
            "Iteration 41000\tTraining step 7749/4000000\t Mean Max Q 5.756591\tepisode total reward 1\t Max episode reward so far 6\t Game length 150\t Game counts 166\n",
            "Iteration 41100\tTraining step 7774/4000000\t Mean Max Q 5.756591\tepisode total reward 2\t Max episode reward so far 6\t Game length 250\t Game counts 166\n",
            "Iteration 41200\tTraining step 7799/4000000\t Mean Max Q 5.754734\tepisode total reward 0\t Max episode reward so far 6\t Game length 69\t Game counts 167\n",
            "Iteration 41300\tTraining step 7824/4000000\t Mean Max Q 5.754734\tepisode total reward 1\t Max episode reward so far 6\t Game length 169\t Game counts 167\n",
            "Iteration 41400\tTraining step 7849/4000000\t Mean Max Q 5.772711\tepisode total reward 0\t Max episode reward so far 6\t Game length 3\t Game counts 168\n",
            "Iteration 41500\tTraining step 7874/4000000\t Mean Max Q 5.772711\tepisode total reward 1\t Max episode reward so far 6\t Game length 103\t Game counts 168\n",
            "Iteration 41600\tTraining step 7899/4000000\t Mean Max Q 5.772711\tepisode total reward 2\t Max episode reward so far 6\t Game length 203\t Game counts 168\n",
            "Iteration 41700\tTraining step 7924/4000000\t Mean Max Q 5.772711\tepisode total reward 3\t Max episode reward so far 6\t Game length 303\t Game counts 168\n",
            "Iteration 41800\tTraining step 7949/4000000\t Mean Max Q 5.776199\tepisode total reward 0\t Max episode reward so far 6\t Game length 66\t Game counts 169\n",
            "Iteration 41900\tTraining step 7974/4000000\t Mean Max Q 5.776199\tepisode total reward 0\t Max episode reward so far 6\t Game length 166\t Game counts 169\n",
            "Iteration 42000\tTraining step 7999/4000000\t Mean Max Q 5.776199\tepisode total reward 2\t Max episode reward so far 6\t Game length 266\t Game counts 169\n",
            "Iteration 42100\tTraining step 8024/4000000\t Mean Max Q 5.796516\tepisode total reward 0\t Max episode reward so far 6\t Game length 83\t Game counts 170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x01nPpCM7ZP",
        "outputId": "3bcfa983-6700-493f-f9ba-54348024d869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEUhcanT9Z3x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}